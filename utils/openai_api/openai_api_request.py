"""
This script is an example of using the OpenAI API to create various interactions with a ChatGLM3 model.
It includes functions to:

1. Conduct a basic chat session, asking about weather conditions in multiple cities.
2. Initiate a simple chat in Chinese, asking the model to tell a short story.
3. Retrieve and print embeddings for a given text input.

Each function demonstrates a different aspect of the API's capabilities, showcasing how to make requests
and handle responses.
"""

from openai import OpenAI

base_url = "http://127.0.0.1:8000/v1/"
client = OpenAI(api_key="EMPTY", base_url=base_url)


def function_chat():
    messages = [{"role": "user", "content": "What's the weather like in San Francisco, Tokyo, and Paris?"}]
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "Get the current weather in a given location",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                        },
                        "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                    },
                    "required": ["location"],
                },
            },
        }
    ]

    response = client.chat.completions.create(
        model="chatglm3",
        messages=messages,
        tools=tools,
        tool_choice="auto",
    )
    if response:
        content = response.choices[0].message.content
        print(content)
    else:
        print("Error:", response.status_code)


def chat(text):
    # å®šä¹‰APIè¯·æ±‚çš„æ•°æ®
    data = {
        "model": "chatglm3-6b",
        "prompt": text,
        "temperature": 0.5,  # æ§åˆ¶è¾“å‡ºç»“æœçš„éšæœºæ€§ï¼ŒèŒƒå›´ä»0.0åˆ°1.0ï¼Œè¶Šé«˜è¶Šéšæœº
        "max_tokens": 75,    # æ§åˆ¶è¾“å‡ºæ–‡æœ¬çš„é•¿åº¦
        "top_p": 1,          # ä¸€ä¸ªæ›´å¤æ‚çš„å‚æ•°ï¼Œä¸temperatureç±»ä¼¼ä½†æ›´åŠ ç²¾ç»†æ§åˆ¶
        "n": 1,              # è¦è¿”å›çš„æœ€å®Œæ•´çš„æ–‡æœ¬æ®µè½æ•°
        "stream": False      # æ˜¯å¦ä»¥æµçš„å½¢å¼è¿”å›è¾“å‡º
    }
    # å‘é€APIè¯·æ±‚
    response = client.chat.completions.create(**data)
    # æ‰“å°å“åº”ç»“æœ
    print(response.get("choices")[0]["text"])

def chat2(text):
    messages = [
        {
            "role": "user",
            "content": text
        }
    ]
    response = client.chat.completions.create(
        model="chatglm3-6b",
        prompt=messages,
        stream=False,
        max_tokens=256,
        temperature=0.8,
        presence_penalty=1.1,
        top_p=0.8)
    if response:
        if False:
            for chunk in response:
                print(chunk.choices[0].delta.content)
        else:
            content = response.choices[0].message.content
            print(content)
    else:
        print("Error:", response.status_code)

def simple_chat(use_stream=True):
    messages = [
        {
            "role": "system",
            "content": "You are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's "
                       "instructions carefully. Respond using markdown.",
        },
        {
            "role": "user",
            "content": "ä½ å¥½ï¼Œè¯·ä½ ç”¨ç”ŸåŠ¨çš„è¯è¯­ç»™æˆ‘è®²ä¸€ä¸ªçŒ«å’Œç‹—çš„å°æ•…äº‹å§"
        }
    ]
    response = client.chat.completions.create(
        model="chatglm3-6b",
        messages=messages,
        stream=use_stream,
        max_tokens=256,
        temperature=0.8,
        presence_penalty=1.1,
        top_p=0.8)
    if response:
        if use_stream:
            for chunk in response:
                print(chunk.choices[0].delta.content)
        else:
            content = response.choices[0].message.content
            print(content)
    else:
        print("Error:", response.status_code)
def chat3(text):
    history = [['ä½ å¥½', 'ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå¸®åˆ°ä½ å‘¢ï¼Ÿ'],['ä½ å¥½ï¼Œç»™æˆ‘è®²ä¸€ä¸ªä¸ƒä»™å¥³çš„æ•…äº‹ï¼Œå¤§æ¦‚20å­—', 'ä¸ƒä¸ªä»™å¥³ä¸‹å‡¡,æ¥åˆ°äººé—´,é‡è§äº†ç‹å­,ç»å†äº†è®¸å¤šå†’é™©å’Œè€ƒéªŒ,æœ€ç»ˆçˆ±æƒ…è·èƒœ']]
    messages=[]
    if history is not None:
        for string in history:
            # æ‰“å°å­—ç¬¦ä¸²
            print(string)
            # for his in string:
            #     print(his)
            i = 0
            for his in string:
                print(his)
                if i==0:
                    dialogue={
                        "role": "user",
                        "content": his
                    }
                elif i==1:
                    dialogue={
                        "role": "assistant",
                        "content": his
                    }
                messages.append(dialogue)
                i = 1
    current = {
            "role": "user",
            "content": text
    }
    messages.append(current)
    print("===============messages=========================")
    print(messages)
    print("===============messages=========================")
    # messages = [
        
    #     {
    #         "role": "user",
    #         "content": text
    #     }
    # ]
    response = client.chat.completions.create(
        model="chatglm3-6b",
        messages=messages,
        stream=False,
        max_tokens=256,
        temperature=0.8,
        presence_penalty=1.1,
        top_p=0.8)
    if response:
        if False:
            for chunk in response:
                print(chunk.choices[0].delta.content)
        else:
            content = response.choices[0].message.content
            print(content)
    else:
        print("Error:", response.status_code)



def embedding():
    response = client.embeddings.create(
        model="bge-large-zh-1.5",
        input=["ä½ å¥½ï¼Œç»™æˆ‘è®²ä¸€ä¸ªæ•…äº‹ï¼Œå¤§æ¦‚100å­—"],
    )
    embeddings = response.data[0].embedding
    print("åµŒå…¥å®Œæˆï¼Œç»´åº¦ï¼š", len(embeddings))


if __name__ == "__main__":
    chat3("ä½ å¥½ï¼Œç»™æˆ‘è®²æ¥šæ±‰ç›¸äº‰çš„æ•…äº‹ï¼Œå¤§æ¦‚20å­—")
    # simple_chat(use_stream=False)
    # simple_chat(use_stream=True)
    # embedding()
    # function_chat()

#     curl -X POST "http://127.0.0.1:8000/v1/chat/completions" \
# -H "Content-Type: application/json" \
# -d "{\"model\": \"chatglm3-6b\", \"messages\": [{\"role\": \"system\", \"content\": \"You are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.\"}, {\"role\": \"user\", \"content\": \"ä½ å¥½ï¼Œç»™æˆ‘è®²ä¸€ä¸ªæ•…äº‹ï¼Œå¤§æ¦‚100å­—\"}], \"stream\": false, \"max_tokens\": 100, \"temperature\": 0.8, \"top_p\": 0.8}"

# curl -X POST "http://127.0.0.1:8000/v1/completions" \
#      -H 'Content-Type: application/json' \
#      -d '{"prompt": "è¯·ç”¨20å­—å†…å›å¤æˆ‘.ä½ ä»Šå¹´å¤šå¤§äº†", "history": []}'
    
# curl -X POST "http://127.0.0.1:8000/v1/completions" \
#      -H 'Content-Type: application/json' \
#      -d '{"prompt": "è¯·ç”¨20å­—å†…å›å¤æˆ‘.ä½ ä»Šå¹´å¤šå¤§äº†", "history": [{"ä½ å¥½","ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"}]}'


# curl -X POST "http://127.0.0.1:8000/v1/completions" \
#      -H 'Content-Type: application/json' \
#      -d '{"prompt": "è¯·ç”¨20å­—å†…å›å¤æˆ‘.ä½ ä»Šå¹´å¤šå¤§äº†", "history": [["ä½ å¥½","ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]]}'


# curl -X POST "http://127.0.0.1:8000/v1/completions" \
#      -H 'Content-Type: application/json' \
#      -d '{"prompt": "è¯·ç”¨20å­—å†…å›å¤æˆ‘.ä½ ä»Šå¹´å¤šå¤§äº†", "history": ["ä½ å¥½"]}'
    
    