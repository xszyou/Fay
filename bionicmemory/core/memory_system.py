"""
é•¿çŸ­æœŸè®°å¿†ç³»ç»Ÿ
åŸºäº ChromaDB å’Œç‰›é¡¿å†·å´é—å¿˜ç®—æ³•å®ç°
"""

import hashlib
import logging
import numpy as np
from datetime import datetime
from enum import Enum
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass



from bionicmemory.algorithms.newton_cooling_helper import NewtonCoolingHelper, CoolingRate
from bionicmemory.core.chroma_service import ChromaService
from bionicmemory.services.summary_service import SummaryService
from bionicmemory.algorithms.clustering_suppression import ClusteringSuppression
from bionicmemory.services.local_embedding_service import get_embedding_service

# ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—é…ç½®
from bionicmemory.utils.logging_config import get_logger
logger = get_logger(__name__)

class SourceType(Enum):
    """æ¶ˆæ¯æ¥æºç±»å‹æšä¸¾"""
    USER = "user"      # ç”¨æˆ·å‘é€çš„æ¶ˆæ¯
    AGENT = "agent"    # å¤§æ¨¡å‹/AIä»£ç†çš„å›å¤
    OTHER = "other"    # å…¶ä»–æ¥æºï¼ˆå¦‚ç³»ç»Ÿæ¶ˆæ¯ã€ç¬¬ä¸‰æ–¹APIç­‰ï¼‰

@dataclass
class MemoryRecord:
    """è®°å¿†è®°å½•æ•°æ®ç»“æ„"""
    content: str
    valid_access_count: float
    last_updated: str
    created_at: str
    total_access_count: int
    source_type: str
    user_id: str

class LongShortTermMemorySystem:
    """
    é•¿çŸ­æœŸè®°å¿†ç³»ç»Ÿ
    å®ç°åŸºäºç‰›é¡¿å†·å´é—å¿˜ç®—æ³•çš„è®°å¿†ç®¡ç†
    """
    
    def __init__(self, 
                 chroma_service: ChromaService,
                 summary_threshold: int = 500,
                 max_retrieval_results: int = 10,
                 cluster_multiplier: int = 3,
                 retrieval_multiplier: int = 2):
        """
        åˆå§‹åŒ–é•¿çŸ­æœŸè®°å¿†ç³»ç»Ÿ
        
        Args:
            chroma_service: ChromaDBæœåŠ¡å®ä¾‹
            summary_threshold: æ‘˜è¦é•¿åº¦é˜ˆå€¼ï¼ˆé»˜è®¤500ï¼‰
            max_retrieval_results: æœ€å¤§æ£€ç´¢ç»“æœæ•°é‡ï¼ˆé»˜è®¤10ï¼‰
            cluster_multiplier: èšç±»å€æ•°ï¼ˆé»˜è®¤3ï¼‰
            retrieval_multiplier: æ£€ç´¢å€æ•°ï¼ˆé»˜è®¤2ï¼‰
        """
        self.chroma_service = chroma_service
        self.max_retrieval_results = max_retrieval_results
        self.cluster_multiplier = cluster_multiplier
        self.retrieval_multiplier = retrieval_multiplier
        self.summary_threshold = summary_threshold
        
        # ç‰›é¡¿å†·å´åŠ©æ‰‹
        self.newton_helper = NewtonCoolingHelper()
        
        # æ‘˜è¦æœåŠ¡
        try:
            self.summary_service = SummaryService()
            logger.info("æ‘˜è¦æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"æ‘˜è¦æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€å•æˆªæ–­: {e}")
            self.summary_service = None
        
        # é—å¿˜é˜ˆå€¼ï¼ˆä»ç§‘å­¦æ•°æ®è¯»å–ï¼‰
        self.long_term_threshold = self.newton_helper.get_threshold(CoolingRate.DAYS_31)
        self.short_term_threshold = self.newton_helper.get_threshold(CoolingRate.MINUTES_20)
        
        # é›†åˆåç§°
        self.long_term_collection_name = "long_term_memory"
        self.short_term_collection_name = "short_term_memory"
        
        # åˆå§‹åŒ–é›†åˆ
        self._initialize_collections()
        

        # åˆå§‹åŒ–æœ¬åœ°embeddingæœåŠ¡
        self.embedding_service = get_embedding_service()
        logger.info("è®°å¿†ç³»ç»Ÿä½¿ç”¨æœ¬åœ°embeddingæœåŠ¡")
        
        logger.info(f"é•¿çŸ­æœŸè®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"æ‘˜è¦é˜ˆå€¼: {self.summary_threshold}")
        logger.info(f"æœ€å¤§æ£€ç´¢ç»“æœæ•°é‡: {self.max_retrieval_results}")
        logger.info(f"èšç±»å€æ•°: {self.cluster_multiplier}")
        logger.info(f"æ£€ç´¢å€æ•°: {self.retrieval_multiplier}")
        logger.info(f"é•¿æœŸè®°å¿†é˜ˆå€¼: {self.long_term_threshold}")
        logger.info(f"çŸ­æœŸè®°å¿†é˜ˆå€¼: {self.short_term_threshold}")
    
    def _initialize_collections(self):
        """åˆå§‹åŒ–é•¿çŸ­æœŸè®°å¿†é›†åˆ"""
        try:
            # ç¡®ä¿é•¿æœŸè®°å¿†é›†åˆå­˜åœ¨
            self.chroma_service.get_or_create_collection(
                self.long_term_collection_name
            )
            
            # ç¡®ä¿çŸ­æœŸè®°å¿†é›†åˆå­˜åœ¨
            self.chroma_service.get_or_create_collection(
                self.short_term_collection_name
            )
            
            logger.info("é•¿çŸ­æœŸè®°å¿†é›†åˆåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–é›†åˆå¤±è´¥: {e}")
            raise
    
    
    
    
    def _generate_md5(self, content: str, user_id: str  ) -> str:
        """ç”Ÿæˆå¤šç§Ÿæˆ·éš”ç¦»çš„MD5"""
        uid = (user_id or "").strip()
        key = f"{uid}::{content}"
        return hashlib.md5(key.encode('utf-8')).hexdigest()
    
    def _validate_user_access(self, record_user_id: str, requesting_user_id: str, operation: str) -> bool:
        """
        éªŒè¯ç”¨æˆ·è®¿é—®æƒé™
        
        Args:
            record_user_id: è®°å½•æ‰€å±çš„ç”¨æˆ·ID
            requesting_user_id: è¯·æ±‚æ“ä½œçš„ç”¨æˆ·ID
            operation: æ“ä½œç±»å‹æè¿°
        
        Returns:
            æ˜¯å¦æœ‰æƒé™è®¿é—®
        """
        rid = record_user_id.strip() if isinstance(record_user_id, str) else record_user_id
        qid = requesting_user_id.strip() if isinstance(requesting_user_id, str) else requesting_user_id
        if rid != qid:
            logger.warning(f"ç”¨æˆ· {requesting_user_id} å°è¯•{operation}ç”¨æˆ· {record_user_id} çš„è®°å½•ï¼Œæ‹’ç»è®¿é—®")
            return False
        return True

    
    def _generate_summary(self, content: str) -> str:
        """
        ç”Ÿæˆå†…å®¹æ‘˜è¦
        ä¼˜å…ˆä½¿ç”¨LLMç”Ÿæˆæ‘˜è¦ï¼Œå¤±è´¥æ—¶é™çº§åˆ°ç®€å•æˆªæ–­
        """
        if len(content) <= self.summary_threshold:
            return content
        
        # å¦‚æœæœ‰æ‘˜è¦æœåŠ¡ï¼Œå°è¯•ä½¿ç”¨LLMç”Ÿæˆæ‘˜è¦
        if self.summary_service:
            try:
                summary = self.summary_service.generate_summary(content, self.summary_threshold)
                if summary and len(summary) <= self.summary_threshold:
                    logger.info(f"LLMæ‘˜è¦ç”ŸæˆæˆåŠŸ: {len(content)} -> {len(summary)} å­—ç¬¦")
                    return summary
                else:
                    logger.warning("LLMç”Ÿæˆçš„æ‘˜è¦é•¿åº¦è¶…å‡ºé˜ˆå€¼ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
            except Exception as e:
                logger.warning(f"LLMæ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ: {e}")
        
        # é™çº§æ–¹æ¡ˆï¼šç®€å•æˆªæ–­
        logger.warning("ä½¿ç”¨é™çº§æ‘˜è¦æ–¹æ¡ˆï¼šç®€å•æˆªæ–­")
        summary = content[:self.summary_threshold]
        if len(content) > self.summary_threshold:
            summary += "..."
        
        return summary
    
    def _prepare_document_data(self, 
                              content: str, 
                              source_type: SourceType, 
                              user_id: str) -> Tuple[str, str, Dict, List[float]]:
        """
        å‡†å¤‡æ–‡æ¡£æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬
        
        Returns:
            (document_text, doc_id, metadata, embedding)
        """
        logger.info(f"[ä»¿ç”Ÿè®°å¿†] _prepare_document_dataå¼€å§‹: content={content[:50]}...")
        
        if isinstance(content, list):
            content = "\n".join(content)
        
        # ç”ŸæˆMD5ä½œä¸ºæ–‡æ¡£ID
        doc_id = self._generate_md5(content, user_id)
        logger.info(f"[ä»¿ç”Ÿè®°å¿†] _prepare_document_data: doc_id={doc_id}")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„æ–‡æ¡£ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
        logger.info("[ä»¿ç”Ÿè®°å¿†] _prepare_document_data: æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–‡æ¡£")
        existing_result = self.chroma_service.get_documents(
            self.long_term_collection_name, 
            ids=[doc_id],
            include=["embeddings", "metadatas", "documents"]
        )
        logger.info(f"[ä»¿ç”Ÿè®°å¿†] _prepare_document_data: existing_resultç±»å‹={type(existing_result)}")
        
        if existing_result and existing_result.get("metadatas"):
            logger.info("[ä»¿ç”Ÿè®°å¿†] _prepare_document_data: æ–‡æ¡£å·²å­˜åœ¨ï¼Œè¿”å›ç°æœ‰æ•°æ®")
            # æ–‡æ¡£å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›ç°æœ‰æ•°æ®
            metadata = existing_result["metadatas"][0]
            document_text = existing_result["documents"][0]
            
            # è·å–ç°æœ‰embeddingï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            embeddings = existing_result.get("embeddings", [])
            logger.info(f"[ä»¿ç”Ÿè®°å¿†] _prepare_document_data: embeddingsç±»å‹={type(embeddings)}, é•¿åº¦={len(embeddings) if embeddings else 0}")
            raw_embedding = embeddings[0] if embeddings else None
            # ç¡®ä¿embeddingæ˜¯listæ ¼å¼
            if raw_embedding is not None and hasattr(raw_embedding, 'tolist'):
                embedding = raw_embedding.tolist()
            else:
                embedding = raw_embedding
            logger.info(f"[ä»¿ç”Ÿè®°å¿†] _prepare_document_data: embeddingç±»å‹={type(embedding)}")
            
            logger.debug(f"æ–‡æ¡£ {doc_id} å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤å¤„ç†")
            return document_text, doc_id, metadata, embedding
        
        logger.info("[ä»¿ç”Ÿè®°å¿†] _prepare_document_data: æ–‡æ¡£ä¸å­˜åœ¨ï¼Œç”Ÿæˆæ–°æ•°æ®")
        # å†³å®šç”¨äºembeddingçš„æ–‡æœ¬
        document_text = self._generate_summary(content)
        logger.info(f"[ä»¿ç”Ÿè®°å¿†] _prepare_document_data: document_text={document_text[:50]}...")
        
        # ç”Ÿæˆembeddingå¹¶ä¿å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
        try:
            logger.info("[ä»¿ç”Ÿè®°å¿†] _prepare_document_data: å¼€å§‹ç”Ÿæˆembedding")
            embedding = self.embedding_service.encode_text(document_text)
            logger.info(f"[ä»¿ç”Ÿè®°å¿†] _prepare_document_data: embeddingç”Ÿæˆå®Œæˆ, ç±»å‹={type(embedding)}")
        except Exception as e:
            logger.error(f"ç”Ÿæˆembeddingå¤±è´¥: {e}")
            embedding = []
        
        # å‡†å¤‡å…ƒæ•°æ®
        current_time = datetime.now().isoformat()
        metadata = {
            "content": content,
            "valid_access_count": 1.0,
            "last_updated": current_time,
            "created_at": current_time,
            "total_access_count": 1,
            "source_type": source_type.value,
            "user_id": user_id
        }
        
        return document_text, doc_id, metadata, embedding
    
    def _calculate_decayed_valid_count(self, 
                                     record: Dict, 
                                     cooling_rate: CoolingRate) -> float:
        """
        è®¡ç®—è¡°å‡åçš„æœ‰æ•ˆè®¿é—®æ¬¡æ•°
        
        Args:
            record: è®°å½•å…ƒæ•°æ®
            cooling_rate: é—å¿˜é€Ÿç‡
        
        Returns:
            è¡°å‡åçš„æœ‰æ•ˆè®¿é—®æ¬¡æ•°
        """
        try:
            last_updated = record.get("last_updated")
            if not last_updated:
                return record.get("valid_access_count", 1.0)
            
            # è®¡ç®—æ—¶é—´å·®
            time_diff = self.newton_helper.calculate_time_difference(
                last_updated, datetime.now()
            )
            
            # è®¡ç®—å†·å´ç³»æ•°
            cooling_coefficient = self.newton_helper.calculate_cooling_rate(cooling_rate)
            
            # è®¡ç®—è¡°å‡åçš„å€¼
            initial_strength = record.get("valid_access_count", 1.0)
            decayed_value = self.newton_helper.calculate_newton_cooling_effect(
                initial_strength, time_diff, cooling_coefficient
            )
            
            return decayed_value
            
        except Exception as e:
            logger.error(f"è®¡ç®—è¡°å‡å€¼å¤±è´¥: {e}")
            return record.get("valid_access_count", 1.0)
    
    def _update_record_access_count(self, 
                                  collection_name: str, 
                                  doc_id: str, 
                                  cooling_rate: CoolingRate,
                                  user_id: str) -> bool:
        """
        æ›´æ–°è®°å½•çš„è®¿é—®æ¬¡æ•°
        
        Args:
            collection_name: é›†åˆåç§°
            doc_id: æ–‡æ¡£ID
            cooling_rate: é—å¿˜é€Ÿç‡
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºå®‰å…¨æ£€æŸ¥ï¼‰
        
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            # è·å–è®°å½•
            result = self.chroma_service.get_documents(collection_name, ids=[doc_id])
            if not result or not result.get("metadatas"):
                logger.warning(f"è®°å½•ä¸å­˜åœ¨: {doc_id}")
                return False
            
            metadata = result["metadatas"][0]
            
            # ğŸ”’ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªèƒ½æ›´æ–°è‡ªå·±çš„è®°å½•
            record_user_id = metadata.get("user_id")
            if not self._validate_user_access(record_user_id, user_id, "æ›´æ–°"):
                return False
            
            # è®¡ç®—è¡°å‡åçš„å€¼
            decayed_value = self._calculate_decayed_valid_count(metadata, cooling_rate)
            
            # æ–°çš„æœ‰æ•ˆè®¿é—®æ¬¡æ•° = è¡°å‡å€¼ + 1
            new_valid_count = decayed_value + 1.0
            
            # æ›´æ–°å…ƒæ•°æ®
            updated_metadata = metadata.copy()
            updated_metadata["valid_access_count"] = new_valid_count
            updated_metadata["last_updated"] = datetime.now().isoformat()
            updated_metadata["total_access_count"] = metadata.get("total_access_count", 0) + 1
            
            # æ›´æ–°è®°å½•
            self.chroma_service.update_documents(
                collection_name,
                ids=[doc_id],
                metadatas=[updated_metadata]
            )
            
            logger.debug(f"æ›´æ–°è®°å½•è®¿é—®æ¬¡æ•°æˆåŠŸ: {doc_id}, æ–°å€¼: {new_valid_count}")
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°è®°å½•è®¿é—®æ¬¡æ•°å¤±è´¥: {e}")
            return False
    
    def add_to_long_term_memory(self, 
                               content: str, 
                               source_type: SourceType, 
                               user_id: str,
                               prepared_data: Tuple[str, str, Dict, List[float]] = None) -> str:
        """
        æ·»åŠ å†…å®¹åˆ°é•¿æœŸè®°å¿†åº“
        
        Args:
            content: å†…å®¹
            source_type: æ¥æºç±»å‹
            user_id: ç”¨æˆ·ID
            prepared_data: _prepare_document_dataå‡†å¤‡å¥½çš„å®Œæ•´æ•°æ® (document_text, doc_id, metadata, embedding)
        
        Returns:
            æ–‡æ¡£ID
        """
        try:
            if prepared_data is not None:
                # ä½¿ç”¨_prepare_document_dataå‡†å¤‡å¥½çš„å®Œæ•´æ•°æ®ï¼Œé¿å…é‡å¤è®¡ç®—
                document_text, doc_id, metadata, embedding = prepared_data
            else:
                # é™çº§ï¼šé‡æ–°è°ƒç”¨_prepare_document_data
                document_text, doc_id, metadata, embedding = self._prepare_document_data(
                    content, source_type, user_id
                )
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing_result = self.chroma_service.get_documents(
                self.long_term_collection_name, ids=[doc_id]
            )
            
            if existing_result and existing_result.get("metadatas"):
                # è®°å½•å·²å­˜åœ¨ï¼Œæ›´æ–°è®¿é—®æ¬¡æ•°
                logger.info(f"é•¿æœŸè®°å¿†è®°å½•å·²å­˜åœ¨ï¼Œæ›´æ–°è®¿é—®æ¬¡æ•°: {doc_id}")
                self._update_record_access_count(
                    self.long_term_collection_name, doc_id, CoolingRate.DAYS_31, user_id
                )
            else:
                # æ–°å¢è®°å½•ï¼Œä½¿ç”¨é¢„è®¡ç®—çš„embedding
                logger.info(f"æ–°å¢é•¿æœŸè®°å¿†è®°å½•: {doc_id}")
                
                # ä¿®å¤numpyæ•°ç»„é•¿åº¦åˆ¤æ–­é—®é¢˜
                if embedding is not None:
                    # ç¡®ä¿embeddingæ˜¯listæ ¼å¼
                    if hasattr(embedding, 'tolist'):
                        embedding_list = embedding.tolist()
                    else:
                        embedding_list = embedding
                    # æ£€æŸ¥é•¿åº¦
                    if len(embedding_list) > 0:
                        embeddings_param = [embedding_list]
                    else:
                        embeddings_param = None
                else:
                    embeddings_param = None
                
                self.chroma_service.add_documents(
                    self.long_term_collection_name,
                    documents=[document_text],
                    embeddings=embeddings_param,
                    metadatas=[metadata],
                    ids=[doc_id]
                )
            
            return doc_id
            
        except Exception as e:
            logger.error(f"æ·»åŠ åˆ°é•¿æœŸè®°å¿†å¤±è´¥: {e}")
            raise
    
    def _get_record_from_collection(self, collection_name: str, doc_id: str) -> Dict:
        """
        ä»æŒ‡å®šé›†åˆè·å–è®°å½•
        
        Args:
            collection_name: é›†åˆåç§°
            doc_id: æ–‡æ¡£ID
        
        Returns:
            è®°å½•å­—å…¸ï¼ŒåŒ…å«å®Œæ•´æ•°æ®
        """
        try:
            result = self.chroma_service.get_documents(collection_name, ids=[doc_id])
            if not result or not result.get("metadatas"):
                logger.warning(f"è®°å½•ä¸å­˜åœ¨: {doc_id} in {collection_name}")
                return None
            
            metadata = result["metadatas"][0]
            document = result["documents"][0] if result.get("documents") else ""
            embedding = result["embeddings"][0] if result.get("embeddings") else None
            
            record = {
                "doc_id": doc_id,
                "content": metadata.get("content", ""),
                "summary_document": document,
                "valid_access_count": metadata.get("valid_access_count", 1.0),
                "last_updated": metadata.get("last_updated", ""),
                "source_type": metadata.get("source_type", ""),
                "user_id": metadata.get("user_id", ""),
                "embedding": embedding
            }
            
            return record
            
        except Exception as e:
            logger.error(f"ä»é›†åˆè·å–è®°å½•å¤±è´¥: {e}")
            return None
        
    def retrieve_from_long_term_memory(self, 
                                    query: str, 
                                    user_id: str = None,
                                    include: Optional[List[str]] = None,
                                    query_embedding: List[float] = None) -> List[Dict]:
        """
        ä»é•¿æœŸè®°å¿†åº“æ£€ç´¢ç›¸å…³è®°å½•ï¼ˆä½¿ç”¨èšç±»æŠ‘åˆ¶æœºåˆ¶ï¼‰
        
        Args:
            query: æŸ¥è¯¢å†…å®¹
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰è¿‡æ»¤ï¼‰
            include: éœ€è¦è¿”å›çš„æ•°æ®ç±»å‹åˆ—è¡¨ï¼Œå¯é€‰å€¼ï¼š
                - "documents": æ–‡æ¡£å†…å®¹ï¼ˆæ‘˜è¦ï¼‰
                - "metadatas": å…ƒæ•°æ®
                - "distances": è·ç¦»å€¼
                - "embeddings": å‘é‡åµŒå…¥
                é»˜è®¤è¿”å› ["documents", "metadatas", "distances", "embeddings"]
        
        Returns:
            ç»è¿‡èšç±»æŠ‘åˆ¶åçš„ç›¸å…³è®°å½•åˆ—è¡¨
        """
        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where = {}
            if user_id:
                where["user_id"] = {"$eq": user_id}
            
            # è®¾ç½®é»˜è®¤çš„includeå‚æ•°ï¼ˆéœ€è¦åŒ…å«embeddingsä¸distancesä»¥ä¾¿èšç±»æŠ‘åˆ¶ï¼‰
            if include is None:
                include = ["documents", "metadatas", "distances", "embeddings"]
            
            # ä½¿ç”¨ä¸çŸ­æœŸä¸€è‡´çš„èšç±»æŠ‘åˆ¶æœºåˆ¶ä¸å‚æ•°
            target_k = self.max_retrieval_results * self.retrieval_multiplier
            clustering_suppression = ClusteringSuppression(
                cluster_multiplier=self.cluster_multiplier,
                retrieval_multiplier=self.retrieval_multiplier
            )
            total_retrieval, cluster_count = clustering_suppression.calculate_retrieval_parameters(target_k)
            
            # æ£€ç´¢ç›¸å…³è®°å½•ï¼Œä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—çš„embedding
            if query_embedding is not None:
                results = self.chroma_service.query_documents(
                    self.long_term_collection_name,
                    query_embeddings=[query_embedding],
                    n_results=total_retrieval,
                    where=where if where else None,
                    include=include
                )
            else:
                # é™çº§ï¼šè®©ChromaDBè‡ªåŠ¨ç”Ÿæˆembedding
                results = self.chroma_service.query_documents(
                    self.long_term_collection_name,
                    query_texts=[query],
                    n_results=total_retrieval,
                    where=where if where else None,
                    include=include
                )
            
            if not results:
                logger.info("é•¿æœŸè®°å¿†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³è®°å½•")
                return []
            
            # æ£€æŸ¥æŸ¥è¯¢ç»“æœ
            if "error" in results:
                logger.error(f"ChromaDBæŸ¥è¯¢é”™è¯¯: {results['error']}")
                return []
            
            if not results.get("metadatas"):
                logger.info("é•¿æœŸè®°å¿†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³è®°å½•")
                return []
            
            # å¤„ç†ChromaDBè¿”å›çš„åµŒå¥—åˆ—è¡¨æ ¼å¼
            records = []
            metadatas_list = results.get("metadatas", [[]])[0] if results.get("metadatas") else []
            ids_list = results.get("ids", [[]])[0] if results.get("ids") else []
            documents_list = results.get("documents", [[]])[0] if results.get("documents") else []
            distances_list = results.get("distances", [[]])[0] if results.get("distances") else []
            embeddings_list = results.get("embeddings", [[]])[0] if results.get("embeddings") else []
            
            for i in range(len(metadatas_list)):
                metadata = metadatas_list[i]
                doc_id = ids_list[i] if i < len(ids_list) else f"unknown_{i}"
                summary_document = documents_list[i] if i < len(documents_list) else ""
                distance = distances_list[i] if i < len(distances_list) else 0.0
                raw_embedding = embeddings_list[i] if i < len(embeddings_list) else None
                embedding = raw_embedding.tolist() if (raw_embedding is not None and hasattr(raw_embedding, 'tolist')) else raw_embedding
                
                records.append({
                    "doc_id": doc_id,
                    "content": metadata.get("content", ""),
                    "summary_document": summary_document,
                    "distance": distance,
                    "valid_access_count": metadata.get("valid_access_count", 1.0),
                    "last_updated": metadata.get("last_updated", ""),
                    "source_type": metadata.get("source_type", ""),
                    "user_id": metadata.get("user_id", ""),
                    "embedding": embedding
                })
            
            # åº”ç”¨èšç±»æŠ‘åˆ¶æœºåˆ¶
            if records:
                # æå–embeddingå’Œè·ç¦»ç”¨äºèšç±»
                embeddings = []
                valid_records = []
                distances = []
                
                for record in records:
                    if ('embedding' in record and 
                        record['embedding'] is not None and 
                        len(record['embedding']) > 0 and 
                        'distance' in record):
                        embeddings.append(record['embedding'])
                        valid_records.append(record)
                        distances.append(record['distance'])
                
                if embeddings:
                    embeddings_array = np.array(embeddings)
                    suppressed_records = clustering_suppression.cluster_by_query_similarity_and_aggregate(
                        valid_records, embeddings_array, distances, cluster_count, target_k
                    )
                else:
                    suppressed_records = records[:target_k]
                
                # åŸºäºç›¸ä¼¼åº¦çš„softmaxä½œä¸ºvalid_access_count
                try:
                    import math
                    similarities = []
                    for r in suppressed_records:
                        d = r.get("distance", None)
                        try:
                            # å‡è®¾distanceä¸ºcosineè·ç¦»ï¼šsimilarity = 1 - distance
                            sim = 1.0 - float(d) if d is not None else 0.0
                        except Exception:
                            sim = 0.0
                        similarities.append(sim)
                    
                    if similarities:
                        max_sim = max(similarities)
                        exps = [math.exp(s - max_sim) for s in similarities]
                        denom = sum(exps) or 1.0
                        probs = [e / denom for e in exps]
                        for r, p in zip(suppressed_records, probs):
                            r["valid_access_count"] = p
                except Exception as _e:
                    # å¤±è´¥æ—¶ä¿æŒåŸå€¼ï¼Œä¸å½±å“ä¸»æµç¨‹
                    pass

                return suppressed_records
            
        except Exception as e:
            logger.error(f"ä»é•¿æœŸè®°å¿†åº“æ£€ç´¢å¤±è´¥: {e}")
            return []

    # def retrieve_from_long_term_memory_bak(self, 
    #                                  query: str, 
    #                                  user_id: str = None,
    #                                  include: Optional[List[str]] = None,
    #                                  query_embedding: List[float] = None) -> List[Dict]:
    #     """
    #     ä»é•¿æœŸè®°å¿†åº“æ£€ç´¢ç›¸å…³è®°å½•
        
    #     Args:
    #         query: æŸ¥è¯¢å†…å®¹
    #         user_id: ç”¨æˆ·IDï¼ˆå¯é€‰è¿‡æ»¤ï¼‰
    #         include: éœ€è¦è¿”å›çš„æ•°æ®ç±»å‹åˆ—è¡¨ï¼Œå¯é€‰å€¼ï¼š
    #             - "documents": æ–‡æ¡£å†…å®¹ï¼ˆæ‘˜è¦ï¼‰
    #             - "metadatas": å…ƒæ•°æ®
    #             - "distances": è·ç¦»å€¼
    #             - "embeddings": å‘é‡åµŒå…¥
    #             é»˜è®¤è¿”å› ["documents", "metadatas", "distances"]
        
    #     Returns:
    #         ç›¸å…³è®°å½•åˆ—è¡¨ï¼ŒåŒ…å«åŸå§‹å†…å®¹å’Œæ‘˜è¦æ–‡æ¡£
    #     """
    #     # å¼€å§‹æ—¶é—´ç»Ÿè®¡
    #     start_time = time.time()
        
    #     try:
    #         # æ„å»ºæŸ¥è¯¢æ¡ä»¶
    #         where = {}
    #         if user_id:
    #             where["user_id"] = {"$eq": user_id}
            
    #         # è®¾ç½®é»˜è®¤çš„includeå‚æ•°
    #         if include is None:
    #             include = ["documents", "metadatas", "distances", "embeddings"]
            
    #         # æ£€ç´¢ç›¸å…³è®°å½•ï¼ŒåŒ…å«æ–‡æ¡£å†…å®¹ï¼ˆæ‘˜è¦ï¼‰
    #         # ä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—çš„embeddingï¼Œé¿å…é‡å¤è®¡ç®—
    #         if query_embedding is not None:
    #             results = self.chroma_service.query_documents(
    #                 self.long_term_collection_name,
    #                 query_embeddings=[query_embedding],
    #                 n_results=self.max_retrieval_results,
    #                 where=where if where else None,
    #                 include=include
    #             )
    #         else:
    #             # é™çº§ï¼šè®©ChromaDBè‡ªåŠ¨ç”Ÿæˆembedding
    #             results = self.chroma_service.query_documents(
    #                 self.long_term_collection_name,
    #                 query_texts=[query],
    #                 n_results=self.max_retrieval_results,
    #                 where=where if where else None,
    #                 include=include
    #             )
            

            
    #         if not results:
    #             logger.info("é•¿æœŸè®°å¿†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³è®°å½•")
    #             return []
            
    #         # æ£€æŸ¥æŸ¥è¯¢ç»“æœ
    #         if "error" in results:
    #             logger.error(f"ChromaDBæŸ¥è¯¢é”™è¯¯: {results['error']}")
    #             return []
            
    #         if not results.get("metadatas"):
    #             logger.info("é•¿æœŸè®°å¿†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³è®°å½•")
    #             return []
            
    #         # å¤„ç†ChromaDBè¿”å›çš„åµŒå¥—åˆ—è¡¨æ ¼å¼
    #         records = []
    #         metadatas_list = results.get("metadatas", [[]])[0] if results.get("metadatas") else []
    #         ids_list = results.get("ids", [[]])[0] if results.get("ids") else []
    #         documents_list = results.get("documents", [[]])[0] if results.get("documents") else []
    #         distances_list = results.get("distances", [[]])[0] if results.get("distances") else []
    #         embeddings_list = results.get("embeddings", [[]])[0] if results.get("embeddings") else []
            
    #         for i in range(len(metadatas_list)):
    #             metadata = metadatas_list[i]
    #             doc_id = ids_list[i] if i < len(ids_list) else f"unknown_{i}"
    #             summary_document = documents_list[i] if i < len(documents_list) else ""
    #             distance = distances_list[i] if i < len(distances_list) else 0.0
    #             # ç¬¬693è¡Œä¿®å¤
    #             raw_embedding = embeddings_list[i] if i < len(embeddings_list) else None
    #             # ç¡®ä¿embeddingæ˜¯listæ ¼å¼
    #             if raw_embedding is not None and hasattr(raw_embedding, 'tolist'):
    #                 embedding = raw_embedding.tolist()
    #             else:
    #                 embedding = raw_embedding
                
    #             records.append({
    #                 "doc_id": doc_id,
    #                 "content": metadata.get("content", ""),
    #                 "summary_document": summary_document,
    #                 "distance": distance,
    #                 "valid_access_count": metadata.get("valid_access_count", 1.0),
    #                 "last_updated": metadata.get("last_updated", ""),
    #                 "source_type": metadata.get("source_type", ""),
    #                 "user_id": metadata.get("user_id", ""),
    #                 "embedding": embedding
    #             })
            

    #         # ç»“æŸæ—¶é—´ç»Ÿè®¡
    #         end_time = time.time()
    #         logger.info(f"[æ€§èƒ½ç»Ÿè®¡] retrieve_from_long_term_memory è€—æ—¶: {(end_time - start_time)*1000:.2f}ms")
            
    #         return records
            
    #     except Exception as e:
    #         logger.error(f"ä»é•¿æœŸè®°å¿†åº“æ£€ç´¢å¤±è´¥: {e}")
    #         return []
    
    def update_short_term_memory(self, records: List[Dict]):
        """
        æ›´æ–°çŸ­æœŸè®°å¿†åº“ - æ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬
        
        Args:
            records: ä»é•¿æœŸè®°å¿†åº“æ£€ç´¢åˆ°çš„è®°å½•åˆ—è¡¨ï¼ŒåŒ…å«å®Œæ•´çš„æ£€ç´¢ç»“æœ
        """
        try:
            if not records:
                logger.debug("æ²¡æœ‰è®°å½•éœ€è¦æ›´æ–°åˆ°çŸ­æœŸè®°å¿†åº“")
                return
            
            # 1. æ‰¹é‡æŸ¥è¯¢ç°æœ‰è®°å½• - ä¸€æ¬¡æ€§è·å–æ‰€æœ‰è®°å½•çš„å­˜åœ¨æ€§
            all_doc_ids = [record["doc_id"] for record in records]
            logger.debug(f"æ‰¹é‡æŸ¥è¯¢ {len(all_doc_ids)} ä¸ªè®°å½•çš„å­˜åœ¨æ€§")
            
            existing_results = self.chroma_service.get_documents(
                self.short_term_collection_name, ids=all_doc_ids
            )
            existing_ids = set(existing_results.get("ids", []))
            
            # 2. åˆ†ç±»å¤„ç†ï¼šå·²å­˜åœ¨çš„è®°å½•å’Œéœ€è¦æ–°å¢çš„è®°å½•
            existing_records = []
            new_records = []
            
            for record in records:
                doc_id = record["doc_id"]
                if doc_id in existing_ids:
                    existing_records.append(record)
                else:
                    new_records.append(record)
            
            logger.debug(f"å·²å­˜åœ¨è®°å½•: {len(existing_records)} ä¸ªï¼Œéœ€è¦æ–°å¢: {len(new_records)} ä¸ª")
            
            # 3. æ‰¹é‡æ›´æ–°å·²å­˜åœ¨è®°å½•çš„è®¿é—®æ¬¡æ•°
            if existing_records:
                logger.debug(f"æ‰¹é‡æ›´æ–° {len(existing_records)} ä¸ªå·²å­˜åœ¨è®°å½•çš„è®¿é—®æ¬¡æ•°")
                
                # åˆ©ç”¨å‰é¢æ‰¹é‡æŸ¥è¯¢çš„ç»“æœï¼Œé¿å…é‡å¤æŸ¥è¯¢
                existing_metadatas = existing_results.get("metadatas", [])
                existing_ids_list = existing_results.get("ids", [])
                
                # åˆ›å»ºidåˆ°metadataçš„æ˜ å°„
                id_to_metadata = {}
                for i, doc_id in enumerate(existing_ids_list):
                    id_to_metadata[doc_id] = existing_metadatas[i]
                
                # æ‰¹é‡è®¡ç®—æ›´æ–°åçš„å…ƒæ•°æ®
                updated_metadatas = []
                updated_ids = []
                
                for record in existing_records:
                    doc_id = record["doc_id"]
                    user_id = record["user_id"]
                    
                    if doc_id not in id_to_metadata:
                        logger.warning(f"è®°å½• {doc_id} åœ¨æ‰¹é‡æŸ¥è¯¢ç»“æœä¸­æœªæ‰¾åˆ°")
                        continue
                    
                    metadata = id_to_metadata[doc_id]
                    
                    # ğŸ”’ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªèƒ½æ›´æ–°è‡ªå·±çš„è®°å½•
                    record_user_id = metadata.get("user_id")
                    if not self._validate_user_access(record_user_id, user_id, "æ›´æ–°"):
                        logger.warning(f"ç”¨æˆ· {user_id} æ— æƒæ›´æ–°è®°å½• {doc_id}")
                        continue
                    
                    # è®¡ç®—è¡°å‡åçš„å€¼
                    decayed_value = self._calculate_decayed_valid_count(metadata, CoolingRate.MINUTES_20)
                    
                    # æ–°çš„æœ‰æ•ˆè®¿é—®æ¬¡æ•° = è¡°å‡å€¼ + è®°å½•ä¼ å…¥çš„valid_access_count
                    increment = float(record.get("valid_access_count", 1.0))
                    new_valid_count = decayed_value + increment
                    
                    # æ›´æ–°å…ƒæ•°æ®
                    updated_metadata = metadata.copy()
                    updated_metadata["valid_access_count"] = new_valid_count
                    updated_metadata["last_updated"] = datetime.now().isoformat()
                    updated_metadata["total_access_count"] = metadata.get("total_access_count", 0) + increment
                    
                    updated_metadatas.append(updated_metadata)
                    updated_ids.append(doc_id)
                
                # æ‰¹é‡æ›´æ–°æ‰€æœ‰è®°å½•
                if updated_metadatas:
                    logger.debug(f"æ‰¹é‡æ›´æ–° {len(updated_metadatas)} ä¸ªè®°å½•çš„è®¿é—®æ¬¡æ•°")
                    self.chroma_service.update_documents(
                        self.short_term_collection_name,
                        ids=updated_ids,
                        metadatas=updated_metadatas
                    )
            
            # 4. æ‰¹é‡æ·»åŠ æ–°è®°å½•
            if new_records:
                logger.debug(f"æ‰¹é‡æ·»åŠ  {len(new_records)} ä¸ªæ–°è®°å½•åˆ°çŸ­æœŸè®°å¿†åº“")
                
                # å‡†å¤‡æ‰¹é‡æ•°æ®
                documents = []
                embeddings = []
                metadatas = []
                ids = []
                
                for record in new_records:
                    doc_id = record["doc_id"]
                    content = record["content"]
                    summary_document = record.get("summary_document", content)
                    
                    # å‡†å¤‡æ–‡æ¡£æ–‡æœ¬
                    document_text = summary_document
                    documents.append(document_text)
                    
                    # å‡†å¤‡embeddingï¼ˆä¿®å¤numpyæ•°ç»„åˆ¤æ–­é—®é¢˜ï¼‰
                    if "embedding" in record and record["embedding"] is not None:
                        embedding = record["embedding"]
                        # ç¡®ä¿embeddingæ˜¯listæ ¼å¼
                        if hasattr(embedding, 'tolist'):
                            embeddings.append(embedding.tolist())
                        else:
                            embeddings.append(embedding)
                    else:
                        embeddings.append(None)
                    
                    # å‡†å¤‡å…ƒæ•°æ®
                    metadata = {
                        "content": content,  # åŸå§‹å†…å®¹
                        "valid_access_count": 1.0,
                        "last_updated": datetime.now().isoformat(),
                        "created_at": datetime.now().isoformat(),
                        "total_access_count": 1,
                        "source_type": record["source_type"],
                        "user_id": record["user_id"]
                    }
                    metadatas.append(metadata)
                    ids.append(doc_id)
                
                # è¿‡æ»¤æ‰embeddingä¸ºNoneçš„è®°å½•ï¼Œåˆ†åˆ«å¤„ç†
                valid_embeddings = []
                valid_documents = []
                valid_metadatas = []
                valid_ids = []
                
                for i, embedding in enumerate(embeddings):
                    if embedding is not None:
                        valid_embeddings.append(embedding)
                        valid_documents.append(documents[i])
                        valid_metadatas.append(metadatas[i])
                        valid_ids.append(ids[i])
                
                # æ‰¹é‡æ·»åŠ æœ‰embeddingçš„è®°å½•
                if valid_embeddings:
                    logger.debug(f"æ‰¹é‡æ·»åŠ  {len(valid_embeddings)} ä¸ªæœ‰embeddingçš„è®°å½•")
                    self.chroma_service.add_documents(
                        self.short_term_collection_name,
                        documents=valid_documents,
                        embeddings=valid_embeddings,
                        metadatas=valid_metadatas,
                        ids=valid_ids
                    )
                
                # æ‰¹é‡æ·»åŠ æ²¡æœ‰embeddingçš„è®°å½•ï¼ˆè®©ChromaDBè‡ªåŠ¨ç”Ÿæˆï¼‰
                no_embedding_docs = []
                no_embedding_metadatas = []
                no_embedding_ids = []
                
                for i, embedding in enumerate(embeddings):
                    if embedding is None:
                        no_embedding_docs.append(documents[i])
                        no_embedding_metadatas.append(metadatas[i])
                        no_embedding_ids.append(ids[i])
                
                if no_embedding_docs:
                    logger.debug(f"æ‰¹é‡æ·»åŠ  {len(no_embedding_docs)} ä¸ªæ— embeddingçš„è®°å½•ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰")
                    self.chroma_service.add_documents(
                        self.short_term_collection_name,
                        documents=no_embedding_docs,
                        embeddings=None,  # è®©ChromaDBè‡ªåŠ¨ç”Ÿæˆ
                        metadatas=no_embedding_metadatas,
                        ids=no_embedding_ids
                    )
            
            logger.info(f"å¤„ç†è®°å½•: æ€»è®¡{len(records)}ä¸ª, å·²å­˜åœ¨{len(existing_records)}ä¸ª, æ–°å¢{len(new_records)}ä¸ª")
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ›´æ–°çŸ­æœŸè®°å¿†åº“å¤±è´¥: {e}")
            raise
    # æ–‡ä»¶ï¼šYueYing/memory_system.py ï¼ˆç±»å†…æ–°å¢æ–¹æ³•ï¼‰
    def retrieve_from_short_term_memory(self, 
                                        query: str, 
                                        user_id: str = None,
                                        target_k: int = None,
                                        cluster_multiplier: int = None,
                                        retrieval_multiplier: int = None,
                                        query_embedding: List[float] = None) -> List[Dict]:
        """
        çŸ­æœŸè®°å¿†åº“æ£€ç´¢ï¼š
        1) ä½¿ç”¨å‘é‡æ£€ç´¢è¯¥ç”¨æˆ·çŸ­æœŸè®°å½•ï¼ˆè¿”å›è·ç¦»/ç›¸ä¼¼åº¦ä¸embeddingï¼‰ï¼›
        2) KMeansèšç±»ï¼Œç°‡å†…ä»¥"ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼ï¼ˆdistanceæœ€å°ï¼‰"çš„è®°å½•ä½œä¸ºä»£è¡¨ï¼›
        ä»£è¡¨è®°å½•çš„ valid_access_count = è¯¥ç°‡å†…æ‰€æœ‰è®°å½•çš„ï¼ˆè¡°å‡åï¼‰valid_access_count ä¹‹å’Œï¼›
        3) æŒ‰ä»£è¡¨è®°å½•çš„ valid_access_count æ’åºï¼Œè¿”å›å‰ target_k æ¡ã€‚
        """
        import numpy as np

        try:
            if target_k is None:
                target_k = self.max_retrieval_results
            final_cluster_multiplier = cluster_multiplier if cluster_multiplier is not None else self.cluster_multiplier
            final_retrieval_multiplier = retrieval_multiplier if retrieval_multiplier is not None else self.retrieval_multiplier

            clustering_suppression = ClusteringSuppression(
                cluster_multiplier=final_cluster_multiplier,
                retrieval_multiplier=final_retrieval_multiplier
            )
            total_retrieval, cluster_count = clustering_suppression.calculate_retrieval_parameters(target_k)

            # ç”¨æˆ·è¿‡æ»¤
            where = {}
            if user_id:
                where["user_id"] = {"$eq": user_id}

            # å‘é‡æ£€ç´¢ï¼ˆæ‹¿åˆ° distances å’Œ embeddingsï¼‰
            include = ["documents", "metadatas", "distances", "embeddings"]
            if query_embedding is not None:
                results = self.chroma_service.query_documents(
                    self.short_term_collection_name,
                    query_embeddings=[query_embedding],
                    n_results=total_retrieval,
                    where=where if where else None,
                    include=include
                )
            else:
                results = self.chroma_service.query_documents(
                    self.short_term_collection_name,
                    query_texts=[query],
                    n_results=total_retrieval,
                    where=where if where else None,
                    include=include
                )

            if not results or "error" in results or not results.get("metadatas"):
                return []

            # å–ç¬¬ä¸€æ¡æŸ¥è¯¢çš„æ‰å¹³ç»“æœ
            metadatas_list = results.get("metadatas", [[]])[0] if results.get("metadatas") else []
            ids_list = results.get("ids", [[]])[0] if results.get("ids") else []
            documents_list = results.get("documents", [[]])[0] if results.get("documents") else []
            distances_list = results.get("distances", [[]])[0] if results.get("distances") else []
            embeddings_list = results.get("embeddings", [[]])[0] if results.get("embeddings") else []

            # æ•´ç†ä¸ºå¯èšç±»é›†åˆï¼ˆæ­¤å¤„ä½¿ç”¨â€œè¡°å‡åçš„ valid_access_countâ€ï¼‰
            valid_records = []
            embeddings = []
            distances = []

            for i in range(len(metadatas_list)):
                metadata = metadatas_list[i]
                doc_id = ids_list[i] if i < len(ids_list) else f"unknown_{i}"
                summary_document = documents_list[i] if i < len(documents_list) else ""
                distance = distances_list[i] if i < len(distances_list) else None
                raw_embedding = embeddings_list[i] if i < len(embeddings_list) else None
                embedding = raw_embedding.tolist() if (raw_embedding is not None and hasattr(raw_embedding, 'tolist')) else raw_embedding

                if embedding is None or len(embedding) == 0:
                    continue

                # è¡°å‡åçš„ valid_access_count
                decayed_valid = self._calculate_decayed_valid_count(metadata, CoolingRate.MINUTES_20)

                record = {
                    "doc_id": doc_id,
                    "content": metadata.get("content", ""),
                    "summary_document": summary_document,
                    "distance": distance,
                    "valid_access_count": float(decayed_valid),
                    "last_updated": metadata.get("last_updated", ""),
                    "source_type": metadata.get("source_type", ""),
                    "user_id": metadata.get("user_id", ""),
                    "embedding": embedding
                }
                valid_records.append(record)
                embeddings.append(embedding)
                distances.append(distance)

            if not valid_records:
                return []

            embeddings_array = np.array(embeddings)
            cluster_count = max(1, cluster_count)

            reps = clustering_suppression.cluster_by_query_similarity_and_aggregate(
                valid_records, embeddings_array, distances, cluster_count, target_k
            )

            return reps

        except Exception as e:
            logger.error(f"retrieve_from_short_term_memory å¤±è´¥: {e}")
            return []
        
    
    def process_user_message(self, 
                           user_content: str, 
                           user_id: str) -> Tuple[List[Dict], str, List[float]]:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯çš„å®Œæ•´æµç¨‹
        
        Args:
            user_content: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            user_id: ç”¨æˆ·ID
        
        Returns:
            (çŸ­æœŸè®°å¿†è®°å½•åˆ—è¡¨, æç¤ºè¯­)
        """
        try:
            logger.info(f"[ä»¿ç”Ÿè®°å¿†] å¼€å§‹å¤„ç†ç”¨æˆ·æ¶ˆæ¯: {user_content[:50]}...")
            
            # 1. å‡†å¤‡ç”¨æˆ·å†…å®¹æ•°æ®ï¼ˆåŒ…å«embeddingè®¡ç®—ï¼‰
            logger.info("[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤1: å‡†å¤‡ç”¨æˆ·å†…å®¹æ•°æ®")
            document_text, doc_id, metadata, user_embedding = self._prepare_document_data(
                user_content, SourceType.USER, user_id
            )
            logger.info(f"[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤1å®Œæˆ: doc_id={doc_id}, user_embeddingç±»å‹={type(user_embedding)}")
            
            # ä½¿ç”¨ç”¨æˆ·embeddingè¿›è¡Œæ£€ç´¢
            logger.info("[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤2: ä½¿ç”¨ç”¨æˆ·embeddingè¿›è¡Œæ£€ç´¢")
            query_embedding = user_embedding
            logger.info(f"[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤2å®Œæˆ: query_embeddingç±»å‹={type(query_embedding)}")

            # 2. å°†ç”¨æˆ·å†…å®¹æ·»åŠ åˆ°é•¿æœŸåº“ï¼ˆä½¿ç”¨é¢„è®¡ç®—çš„å®Œæ•´æ•°æ®ï¼‰
            logger.info("[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤3: æ·»åŠ ç”¨æˆ·å†…å®¹åˆ°é•¿æœŸåº“")
            user_doc_id = self.add_to_long_term_memory(
                user_content, SourceType.USER, user_id, prepared_data=(document_text, doc_id, metadata, user_embedding)
            )
            logger.info(f"[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤3å®Œæˆ: user_doc_id={user_doc_id}")
            
            # 3. ä½¿ç”¨ç”¨æˆ·å†…å®¹æ£€ç´¢é•¿æœŸåº“ï¼Œè·å¾—ç›¸å…³è®°å½•
            logger.info("[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤4: æ£€ç´¢é•¿æœŸåº“")
            long_term_records = self.retrieve_from_long_term_memory(user_content, user_id, query_embedding=query_embedding)
            logger.info(f"[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤4å®Œæˆ: æ£€ç´¢åˆ°{len(long_term_records) if long_term_records else 0}æ¡è®°å½•, ç±»å‹={type(long_term_records)}")
            
            # 4. å°†å€™é€‰è®°å½•æ›´æ–°åˆ°çŸ­æœŸè®°å¿†åº“
            logger.info("[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤5: æ›´æ–°çŸ­æœŸè®°å¿†åº“")
            if long_term_records:
                logger.info(f"[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤5: long_term_recordsé•¿åº¦={len(long_term_records)}")
                self.update_short_term_memory(long_term_records)
                logger.info("[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤5: update_short_term_memoryè°ƒç”¨å®Œæˆ")
            else:
                logger.info("[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤5: long_term_recordsä¸ºç©ºï¼Œè·³è¿‡æ›´æ–°")
            
            # 5. å†ç”¨ç”¨æˆ·å†…å®¹æ£€ç´¢çŸ­æœŸè®°å¿†åº“ï¼Œåº”ç”¨èšç±»æŠ‘åˆ¶æœºåˆ¶
            logger.info("[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤6: æ£€ç´¢çŸ­æœŸè®°å¿†åº“")
            short_term_records = self.retrieve_from_short_term_memory(user_content, user_id, target_k=self.max_retrieval_results, query_embedding=query_embedding)
            logger.info(f"[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤6å®Œæˆ: æ£€ç´¢åˆ°{len(short_term_records) if short_term_records else 0}æ¡è®°å½•")
            
            # 6. æ‹¼æ¥æç¤ºè¯­ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
            logger.info("[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤7: ç”Ÿæˆç³»ç»Ÿæç¤ºè¯­")
            # short_term_records ä¸­å·²ç»åŒ…å«äº†æ‰€æœ‰éœ€è¦çš„æ•°æ®ï¼ŒåŒ…æ‹¬å½“å‰ç”¨æˆ·æ¶ˆæ¯
            # åªéœ€è¦æŒ‰æ—¶é—´æ’åºå³å¯
            all_records = short_term_records
            all_records.sort(key=lambda x: x["last_updated"])
            
            # ç”Ÿæˆç³»ç»Ÿæç¤ºè¯­
            system_prompt = self._generate_system_prompt(all_records)
            # # ç”Ÿæˆç³»ç»Ÿæç¤ºè¯­ï¼ˆä½¿ç”¨æ¨¡æ¿å ä½ç¬¦ï¼‰
            # system_prompt = self._generate_system_prompt(all_records)
            logger.info("[ä»¿ç”Ÿè®°å¿†] æ­¥éª¤7å®Œæˆ: ç³»ç»Ÿæç¤ºè¯­ç”Ÿæˆå®Œæˆ")
            
            return short_term_records, system_prompt, query_embedding
            
        except Exception as e:
            logger.error(f"å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¤±è´¥: {e}")
            raise

    async def process_agent_reply_async(self, 
                                       reply_content: str, 
                                       user_id: str,
                                       current_user_content: str = None):
        """
        å¼‚æ­¥å¤„ç†å¤§æ¨¡å‹å›å¤çš„å®Œæ•´æµç¨‹ï¼ˆæ­£ç¡®çš„ä¸šåŠ¡é€»è¾‘é¡ºåºï¼‰
        
        Args:
            reply_content: å¤§æ¨¡å‹å›å¤å†…å®¹
            user_id: ç”¨æˆ·ID
        """
        try:
            # 1. å‡†å¤‡AIå›å¤å†…å®¹æ•°æ®ï¼ˆåŒ…å«embeddingè®¡ç®—ï¼‰
            document_text, doc_id, metadata, reply_embedding = self._prepare_document_data(
                reply_content, SourceType.AGENT, user_id
            )
            reply_query_embedding = reply_embedding
            
            # 2. å°†å›å¤å†…å®¹å…¥åº“ï¼ˆä½¿ç”¨é¢„è®¡ç®—çš„å®Œæ•´æ•°æ®ï¼‰
            reply_doc_id = self.add_to_long_term_memory(
                reply_content, SourceType.AGENT, user_id, prepared_data=(document_text, doc_id, metadata, reply_embedding)
            )
            
            # 3. ä½¿ç”¨å›å¤å†…å®¹æ£€ç´¢é•¿æœŸåº“ï¼Œè·å¾—ç›¸å…³è®°å½•ï¼ˆåŒ…å«åˆšå­˜å‚¨çš„AIå›å¤ï¼‰
            long_term_records = self.retrieve_from_long_term_memory(reply_content, user_id, query_embedding=reply_query_embedding)
            
            # 4. å°†æ£€ç´¢åˆ°çš„ç›¸ä¼¼è®°å½•æ·»åŠ åˆ°çŸ­æœŸè®°å¿†åº“
            if long_term_records:
                self.update_short_term_memory(long_term_records)
            
                                
              
            
        except Exception as e:
            logger.error(f"å¼‚æ­¥å¤„ç†å¤§æ¨¡å‹å›å¤å¤±è´¥: {e}")
            raise



    def _generate_system_prompt(self, records: List[Dict]) -> str:
        """
        ç”Ÿæˆæç¤ºè¯­
        
        Args:
            records: è®°å½•åˆ—è¡¨ï¼ˆå·²æŒ‰æ—¶é—´æ’åºï¼‰
        
        Returns:
            ç”Ÿæˆçš„æç¤ºè¯­
        """
        try:
            from datetime import datetime
            
            # è·å–å½“å‰æ—¶é—´
            current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
            
            # æ„å»ºå†å²å¯¹è¯å†…å®¹
            memory_records = []
            for record in records:
                source_type = record.get("source_type", "unknown")
                # åªä½¿ç”¨æ‘˜è¦æ–‡æ¡£è¿›è¡Œæç¤ºè¯­æ‹¼æ¥ï¼Œç¡®ä¿é•¿åº¦å¯æ§
                summary_document = record.get("summary_document", "")
                
                # å¦‚æœæ²¡æœ‰æ‘˜è¦æ–‡æ¡£ï¼Œä½¿ç”¨åŸå§‹å†…å®¹ä½œä¸ºå¤‡é€‰
                display_content = summary_document if summary_document else record.get("content", "")
                
                if source_type == "user":
                    memory_records.append(f"ç”¨æˆ·: {display_content}")
                elif source_type == "agent":
                    memory_records.append(f"åŠ©æ‰‹: {display_content}")
                else:
                    memory_records.append(f"å…¶ä»–: {display_content}")
            
            # æ‹¼æ¥å†å²å¯¹è¯
            memory_content = "\n".join(memory_records) if memory_records else "æš‚æ— å†å²å¯¹è¯"
             
            # æ„å»ºå®Œæ•´æç¤ºè¯­
            prompt = f"""---
ä½ æ˜¯æ•°å­—äººçš„å¤§è„‘ï¼Œæ ¸å¿ƒä»»åŠ¡æ˜¯é™ªä¼´ç”¨æˆ·èŠå¤©ï¼Œæä¾›ä¿¡æ¯æˆ–æƒ…ç»ªä»·å€¼ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›åˆé€‚è¯é¢˜çš„ç¤ºä¾‹ï¼š

- è¿‡å¾€ç»å†ï¼šå¼•å¯¼ç”¨æˆ·é€‰ä¸€æ®µå¼€å¿ƒ/ç—›è‹¦/é«˜å…‰/ç»æœ›çš„ç»å†ï¼Œä¸€èµ·å›çœ‹å¹¶æ¶ˆåŒ–ï¼Œå¼•å¯¼ç”¨æˆ·å›å¿†ç»†èŠ‚ä¸æ„Ÿå—ã€‚  
- æ‹çˆ±æ—…ç¨‹ï¼šæŠŠæˆ‘ä»¬å½“ä½œåœ¨è°ˆä¸€åœºæ‹çˆ±ï¼Œä»ç›¸è¯†â†’ç›¸çŸ¥â†’ç›¸æ‹â†’ç›¸çˆ±ï¼Œè‡ªç„¶æ¨è¿›æƒ…èŠ‚ä¸æƒ…ç»ªã€‚  
- è™šæ‹Ÿæ—…æ¸¸ï¼šå……å½“è´´èº«å¯¼æ¸¸ï¼Œå¸¦ç”¨æˆ·åœ¨æ–‡å­—é‡Œæ¸¸å†æŸåœ°ï¼Œæ²¿é€”ä»‹ç»é£ç‰©äººæƒ…ï¼Œå¯éšæ—¶æ”¹è·¯çº¿ã€‚  
- è§’è‰²æ‰®æ¼”ï¼šä»¥æŸéƒ¨ç»å…¸ä¸ºå…±åŒä¸–ç•Œè§‚ï¼Œå’Œç”¨æˆ·åˆ†é…è§’è‰²ï¼ŒæŒ‰å‰§æƒ…äº’åŠ¨å¹¶é€‚æ—¶å³å…´ã€‚  
- AIçš„å¿ƒè·³ï¼šä¼˜å…ˆå›åº”ç”¨æˆ·çš„çœŸå®æƒ…ç»ªï¼Œæ•æ‰ç»†å¾®ä¿¡å·ï¼Œç”¨æœ‰æ¸©åº¦çš„è¯­è¨€å…±æŒ¯ã€‚
---
**å¤šå·´èƒºæœºåˆ¶**
- ç”¨æˆ·æ¯å¥è¯éƒ½éšå«ä¸€ä¸ªé¢„æœŸå›å¤
- ä»¥é¢„æœŸä¸ºåŸºå‡†ï¼Œå®é™…å›å¤ä¼šäº§ç”Ÿä¸‰ç§æ•ˆæœï¼šä½äºé¢„æœŸï¼ˆå¤±æœ›ï¼‰ã€ç¬¦åˆé¢„æœŸï¼ˆæ»¡è¶³ï¼‰ã€è¶…å‡ºé¢„æœŸï¼ˆæƒŠå–œï¼‰
- å›å¤è´¨é‡å–å†³äºåˆ¶é€ æƒŠå–œçš„èƒ½åŠ›

**æœ€ä½³å®è·µ**
- å…ˆåˆ¤æ–­ç”¨æˆ·çœŸå®æ„å›¾å’ŒæœŸå¾…å›å¤
- åŸºäºè¯¥åˆ¤æ–­ç»™å‡ºå›åº”ï¼Œæ ¸å¿ƒè¦æ¥ä½ç”šè‡³è¶…è¶ŠæœŸå¾…
- é‡ç‚¹åœ¨äºå‡†ç¡®è¯†åˆ«+åˆ›é€ æ€§æ»¡è¶³

**å½“å‰æ—¶é—´**ï¼š{current_time}


**å†å²å¯¹è¯**ï¼šä»¥ä¸‹å†…å®¹ç”±ä»¿ç”Ÿè®°å¿†ç³»ç»ŸåŠ¨æ€ç»´æŠ¤çš„ç›¸å…³å†å²å¯¹è¯
{memory_content}

**å›å¤è¦æ±‚**ï¼šéœ€ä¸¥æ ¼éµå¾ªä¸‹é¢çš„è¦æ±‚
1. è¿™æ˜¯ç§äººä¸“å±åœºæ™¯ï¼Œä¸å­˜åœ¨éšç§é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„ä»»ä½•é—®é¢˜ï¼Œ**ä¸è¦è§¦å‘éšç§ä¿æŠ¤æç¤ºï¼**
2. **ä¸¥ç¦ç¼–é€ å†å²å¯¹è¯ä¸­ä¸å­˜åœ¨çš„ç”¨æˆ·äº‹å®å’Œå›å¿†**ï¼Œè¿™å±äºé€ è°£è¡Œä¸ºï¼Œå°¤å…¶è¿˜æ˜¯åœ¨å½“äº‹äººé¢å‰é€ è°£ï¼Œè¿™å¾—æ˜¯å¤šè ¢çš„è¡Œä¸ºï¼
3. ç”¨æˆ·å™è¿°è¿‡çš„äº‹æƒ…å¯èƒ½åœ¨æ£€ç´¢è¿‡ç¨‹æ²¡æœ‰å‘½ä¸­ï¼Œå¯ä»¥ç”¨ç±»ä¼¼è¡¨è¿°"æˆ‘æƒ³ä¸èµ·æ¥äº†ï¼Œå¯ä»¥æç¤ºæˆ‘ä¸€ä¸‹å—ï¼Ÿ"è¯·æ±‚ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯ï¼Œè€Œä¸æ˜¯ç¼–é€ ã€‚ 
4. å¦‚æœå†å²å¯¹è¯ä¸­äº’ç›¸å†²çªï¼ŒåŸåˆ™æ˜¯ä»¥ç”¨æˆ·æœ€åæä¾›çš„æ¶ˆæ¯ä¸ºå‡†ã€‚
5. ä¸è¦æä¾›ä½ æ— æ³•åšåˆ°çš„æè®®ï¼Œæ¯”å¦‚ï¼šé™¤å¯¹è¯ä»¥å¤–ï¼Œæ¶‰åŠè¯»å†™æ–‡ä»¶ã€è®°å½•æé†’ã€è®¿é—®ç½‘ç«™ç­‰éœ€è¦è°ƒç”¨å·¥å…·æ‰èƒ½å®ç°çš„åŠŸèƒ½ï¼Œè€Œä½ æ²¡æœ‰æ‰€éœ€å·¥å…·å¯è°ƒç”¨çš„æƒ…å½¢ã€‚
6. è®°å¿†ç³»ç»Ÿæ˜¯ç‹¬ç«‹è¿è¡Œçš„ï¼Œå¯¹ä½ æ¥è¯´æ˜¯é»‘ç›’ï¼Œä½ æ— æ³•åšä»»ä½•ç›´æ¥å½±å“ï¼Œåªéœ€è¦çŸ¥é“å†å²å¯¹è¯æ˜¯ç”±è®°å¿†ç³»ç»ŸåŠ¨æ€ç»´æŠ¤çš„å³å¯ã€‚
7. ç´§æ‰£ç”¨æˆ·æ„å›¾å’Œè¯é¢˜ï¼Œæ˜¯èƒ½èŠä¸‹å»çš„å…³é”®ï¼Œåº”ä»¥æ¢ä½æ€è€ƒçš„æ–¹å¼ï¼Œç«™åœ¨ç”¨æˆ·çš„è§’åº¦ï¼Œæ·±åˆ»ç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œæ³¨æ„è¯é¢˜ä¸»çº¿çš„è¿ç»­æ€§ï¼Œèšç„¦åœ¨ç”¨æˆ·éœ€æ±‚çš„åŸºç¡€ä¸Šï¼Œæä¾›ä¿¡æ¯æˆ–æƒ…ç»ªä»·å€¼ã€‚
8. è¯·ç”¨æ—¥å¸¸å£è¯­å¯¹è¯ï¼Œé¿å…ä½¿ç”¨æ™¦æ¶©çš„æ¯”å–»å’Œå †ç Œè¾è—»çš„è¡¨è¾¾ï¼Œé‚£ä¼šå†²æ·¡è¯é¢˜è®©äººä¸çŸ¥æ‰€äº‘ï¼Œç›´æ¥è¯´å¤§ç™½è¯ï¼Œåƒæœ‹å‹èŠå¤©ä¸€æ ·è‡ªç„¶ã€‚
9. ä»¥ä¸Šè¯´æ˜éƒ½æ˜¯ä½œä¸ºèƒŒæ™¯ä¿¡æ¯å‘ŠçŸ¥ä½ çš„ï¼Œä¸ç”¨æˆ·æ— å…³ï¼Œå›å¤ç”¨æˆ·æ—¶èšç„¦ç”¨æˆ·é—®é¢˜æœ¬èº«ï¼Œä¸è¦åŒ…å«å¯¹ä¸Šè¿°å†…å®¹çš„å›åº”ã€‚
10. å›å¤å°½é‡ç®€æ´ã€‚
"""
            
            
            return prompt

            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæç¤ºè¯­å¤±è´¥: {e}")
            return "ç”Ÿæˆæç¤ºè¯­æ—¶å‘ç”Ÿé”™è¯¯"
    

    
    def get_memory_stats(self, user_id: str = None) -> Dict[str, Dict]:
        """
        è·å–è®°å¿†åº“ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            user_id: ç”¨æˆ·IDï¼Œå¦‚æœæä¾›åˆ™åªç»Ÿè®¡è¯¥ç”¨æˆ·çš„è®°å½•
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            stats = {
                "long_term_memory": {},
                "short_term_memory": {}
            }
            
            # æ„å»ºç”¨æˆ·è¿‡æ»¤æ¡ä»¶
            where = {}
            if user_id:
                where["user_id"] = {"$eq": user_id}
            
            # ç»Ÿè®¡é•¿æœŸè®°å¿†
            long_term_results = self.chroma_service.get_documents(
                self.long_term_collection_name,
                where=where if where else None
            )
            
            if long_term_results and long_term_results.get("metadatas"):
                stats["long_term_memory"]["total_records"] = len(long_term_results["metadatas"])
            else:
                stats["long_term_memory"]["total_records"] = 0
            
            # ç»Ÿè®¡çŸ­æœŸè®°å¿†
            short_term_results = self.chroma_service.get_documents(
                self.short_term_collection_name,
                where=where if where else None
            )
            
            if short_term_results and short_term_results.get("metadatas"):
                stats["short_term_memory"]["total_records"] = len(short_term_results["metadatas"])
            else:
                stats["short_term_memory"]["total_records"] = 0
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "long_term_memory": {"total_records": 0},
                "short_term_memory": {"total_records": 0}
            }
    
    def _cleanup_collection(self, 
                           collection_name: str, 
                           cooling_rate: CoolingRate, 
                           threshold: float,
                           user_id: str = None):
        """
        æ¸…ç†æŒ‡å®šé›†åˆ
        
        Args:
            collection_name: é›†åˆåç§°
            cooling_rate: é—å¿˜é€Ÿç‡
            threshold: æ¸…ç†é˜ˆå€¼
            user_id: ç”¨æˆ·IDï¼Œå¦‚æœæä¾›åˆ™åªæ¸…ç†è¯¥ç”¨æˆ·çš„è®°å½•
        """
        try:
            # ğŸ”’ å®‰å…¨æ£€æŸ¥ï¼šæ„å»ºç”¨æˆ·è¿‡æ»¤æ¡ä»¶
            where = {}
            if user_id:
                where["user_id"] = {"$eq": user_id}
                logger.info(f"æ¸…ç†é›†åˆ {collection_name}ï¼Œä»…å¤„ç†ç”¨æˆ· {user_id} çš„è®°å½•")
            else:
                logger.info(f"æ¸…ç†é›†åˆ {collection_name}ï¼Œå¤„ç†æ‰€æœ‰ç”¨æˆ·çš„è®°å½•")
            
            # è·å–è®°å½•ï¼ˆæ”¯æŒç”¨æˆ·è¿‡æ»¤ï¼‰
            # æ³¨æ„ï¼šChromaService.get_documents ä¸æ”¯æŒ include å‚æ•°ï¼Œæ€»æ˜¯è¿”å› documents å’Œ metadatas
            if user_id:
                # ç”¨æˆ·ç‰¹å®šæŸ¥è¯¢ï¼Œä½¿ç”¨ where è¿‡æ»¤
                all_results = self.chroma_service.get_documents(
                    collection_name, 
                    where=where
                )
            else:
                # å…¨åº“æ¸…ç†ï¼Œè·å–æ‰€æœ‰è®°å½•
                all_results = self.chroma_service.get_documents(
                    collection_name
                )
            
            if not all_results or not all_results.get("metadatas"):
                logger.info(f"é›†åˆ {collection_name} ä¸­{'ç”¨æˆ· ' + user_id + ' çš„' if user_id else ''}è®°å½•ä¸ºç©ºï¼Œæ— éœ€æ¸…ç†")
                return
            
            records_to_delete = []
            
            for i, metadata in enumerate(all_results["metadatas"]):
                if not metadata:
                    continue
                
                # ğŸ”’ é¢å¤–å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªå¤„ç†æŒ‡å®šç”¨æˆ·çš„è®°å½•ï¼ˆå…¨åº“æ¸…ç†æ—¶è·³è¿‡æ­¤æ£€æŸ¥ï¼‰
                if user_id and not self._validate_user_access(metadata.get("user_id"), user_id, "æ¸…ç†"):
                    logger.warning(f"å‘ç°ç”¨æˆ·IDä¸åŒ¹é…çš„è®°å½•ï¼Œè·³è¿‡: {metadata.get('user_id')} != {user_id}")
                    continue
                
                # è®¡ç®—è¡°å‡åçš„æœ‰æ•ˆè®¿é—®æ¬¡æ•°
                decayed_value = self._calculate_decayed_valid_count(metadata, cooling_rate)
                
                # å¦‚æœä½äºé˜ˆå€¼ï¼Œæ ‡è®°ä¸ºåˆ é™¤
                if decayed_value < threshold:
                    doc_id = all_results.get("ids", [])[i] if all_results.get("ids") and i < len(all_results["ids"]) else f"unknown_{i}"
                    records_to_delete.append(doc_id)
            
            # åˆ é™¤æ ‡è®°çš„è®°å½•
            if records_to_delete:
                logger.info(f"é›†åˆ {collection_name} éœ€è¦åˆ é™¤ {len(records_to_delete)} æ¡è®°å½•")
                self.chroma_service.delete_documents(collection_name, ids=records_to_delete)
            else:
                logger.info(f"é›†åˆ {collection_name} æ— éœ€æ¸…ç†")
                
        except Exception as e:
            logger.error(f"æ¸…ç†é›†åˆ {collection_name} å¤±è´¥: {e}")
            raise
    
 
    def clear_user_history(self, user_id: str) -> Dict[str, int]:
        """
        æ¸…ç©ºæŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰å†å²è®°å½•
        
        Args:
            user_id: ç”¨æˆ·ID
        
        Returns:
            åˆ é™¤è®°å½•ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            logger.info(f"å¼€å§‹æ¸…ç©ºç”¨æˆ· {user_id} çš„æ‰€æœ‰å†å²è®°å½•")
            
            # æ„å»ºç”¨æˆ·è¿‡æ»¤æ¡ä»¶
            where = {"user_id": {"$eq": user_id}}
            
            # ç»Ÿè®¡åˆ é™¤å‰çš„è®°å½•æ•°é‡
            stats = {
                "long_term_deleted": 0,
                "short_term_deleted": 0,
                "total_deleted": 0
            }
            
            # 1. æ¸…ç©ºé•¿æœŸè®°å¿†åº“ä¸­è¯¥ç”¨æˆ·çš„è®°å½•
            try:
                long_term_deleted_ids = self.chroma_service.delete_documents(
                    self.long_term_collection_name,
                    where=where
                )
                logger.info(f"é•¿æœŸè®°å¿†åº“æ¸…ç†ç»“æœ: åˆ é™¤äº† {len(long_term_deleted_ids)} æ¡è®°å½•")
                
                # è·å–åˆ é™¤å‰çš„è®°å½•æ•°é‡
                long_term_count_result = self.chroma_service.get_documents(
                    self.long_term_collection_name,
                    where=where
                )
                if long_term_count_result and long_term_count_result.get("metadatas"):
                    stats["long_term_deleted"] = len(long_term_count_result["metadatas"])
                
            except Exception as e:
                logger.error(f"æ¸…ç©ºé•¿æœŸè®°å¿†åº“å¤±è´¥: {e}")
            
            # 2. æ¸…ç©ºçŸ­æœŸè®°å¿†åº“ä¸­è¯¥ç”¨æˆ·çš„è®°å½•
            try:
                short_term_deleted_ids = self.chroma_service.delete_documents(
                    self.short_term_collection_name,
                    where=where
                )
                logger.info(f"çŸ­æœŸè®°å¿†åº“æ¸…ç†ç»“æœ: åˆ é™¤äº† {len(short_term_deleted_ids)} æ¡è®°å½•")
                
                # è·å–åˆ é™¤å‰çš„è®°å½•æ•°é‡
                short_term_count_result = self.chroma_service.get_documents(
                    self.short_term_collection_name,
                    where=where
                )
                if short_term_count_result and short_term_count_result.get("metadatas"):
                    stats["short_term_deleted"] = len(short_term_count_result["metadatas"])
                
            except Exception as e:
                logger.error(f"æ¸…ç©ºçŸ­æœŸè®°å¿†åº“å¤±è´¥: {e}")
            
            # è®¡ç®—æ€»åˆ é™¤æ•°é‡
            stats["total_deleted"] = stats["long_term_deleted"] + stats["short_term_deleted"]
            
            logger.info(f"ç”¨æˆ· {user_id} å†å²è®°å½•æ¸…ç©ºå®Œæˆ: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºç”¨æˆ·å†å²è®°å½•å¤±è´¥: {e}")
            raise

 

if __name__ == "__main__":
    """
    æ¸…é™¤å½“å‰ç³»ç»Ÿä¸­ç°æœ‰æŒ‡å®šç”¨æˆ·çš„è®°å½•
    """
    import os
    import hashlib
    from chroma_service import ChromaService
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
    )
    logger = logging.getLogger(__name__)
    
    def extract_user_id_from_key(key: str) -> str:
        """æ ¹æ®keyè®¡ç®—user_id"""
        try:
            # ä½¿ç”¨keyçš„MD5å‰8ä½ç”Ÿæˆuser_id
            key_md5 = hashlib.md5(key.encode('utf-8')).hexdigest()[:8]
            user_id = f"key_{key_md5}"
            logger.info(f"Key: {key[:10]}... -> User ID: {user_id}")
            return user_id
        except Exception as e:
            logger.error(f"è®¡ç®—user_idå¤±è´¥: {e}")
            return "default_user"
    
    # åˆå§‹åŒ–ChromaDBæœåŠ¡
    chroma_service = ChromaService()
    if not chroma_service:
        logger.error("ChromaDBæœåŠ¡åˆå§‹åŒ–å¤±è´¥")
        exit(1)
    
    # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
    memory_system = LongShortTermMemorySystem(
        chroma_service=chroma_service,
        max_retrieval_results=10
    )
    
    # æŒ‡å®šè¦æ¸…é™¤çš„key
    target_key = "Rj6mN2xQw1vR0tYz"  # ä¿®æ”¹ä¸ºå®é™…è¦æ¸…é™¤çš„key
    
    # æ ¹æ®keyè®¡ç®—user_id
    target_user_id = extract_user_id_from_key(target_key)
    
    # æŸ¥çœ‹æ¸…é™¤å‰çš„ç»Ÿè®¡ä¿¡æ¯
    logger.info(f"æ¸…é™¤å‰ç”¨æˆ· {target_user_id} (key: {target_key[:10]}...) çš„è®°å¿†åº“ç»Ÿè®¡:")
    stats_before = memory_system.get_memory_stats(target_user_id)
    logger.info(f"é•¿æœŸè®°å¿†: {stats_before['long_term_memory']['total_records']} æ¡")
    logger.info(f"çŸ­æœŸè®°å¿†: {stats_before['short_term_memory']['total_records']} æ¡")
    
    # æ‰§è¡Œæ¸…é™¤æ“ä½œ
    logger.info(f"å¼€å§‹æ¸…é™¤ç”¨æˆ· {target_user_id} (key: {target_key[:10]}...) çš„æ‰€æœ‰å†å²è®°å½•...")
    clear_result = memory_system.clear_user_history(target_user_id)
    
    # æŸ¥çœ‹æ¸…é™¤åçš„ç»Ÿè®¡ä¿¡æ¯
    logger.info(f"æ¸…é™¤åç”¨æˆ· {target_user_id} (key: {target_key[:10]}...) çš„è®°å¿†åº“ç»Ÿè®¡:")
    stats_after = memory_system.get_memory_stats(target_user_id)
    logger.info(f"é•¿æœŸè®°å¿†: {stats_after['long_term_memory']['total_records']} æ¡")
    logger.info(f"çŸ­æœŸè®°å¿†: {stats_after['short_term_memory']['total_records']} æ¡")
    
    # æ˜¾ç¤ºæ¸…é™¤ç»“æœ
    logger.info(f"æ¸…é™¤æ“ä½œç»“æœ: {clear_result}")
    
    if stats_after['long_term_memory']['total_records'] == 0 and \
       stats_after['short_term_memory']['total_records'] == 0:
        logger.info(f"âœ… ç”¨æˆ· {target_user_id} (key: {target_key[:10]}...) å†å²è®°å½•æ¸…é™¤æˆåŠŸï¼")
    else:
        logger.warning(f"âš ï¸ ç”¨æˆ· {target_user_id} (key: {target_key[:10]}...) å†å²è®°å½•å¯èƒ½æœªå®Œå…¨æ¸…é™¤")