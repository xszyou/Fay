"""
åŸºäºOpenAIå®˜æ–¹åº“çš„ä»£ç†æœåŠ¡å™¨
ä½¿ç”¨OpenAIå®˜æ–¹å®¢æˆ·ç«¯å¤„ç†æ‰€æœ‰è¯·æ±‚ï¼Œç¡®ä¿å®Œå…¨å…¼å®¹
"""

from contextlib import asynccontextmanager
import os
import json
import logging
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from fastapi import FastAPI, Request, Response, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
import uvicorn

# OpenAIå®˜æ–¹åº“
from openai import OpenAI, AsyncOpenAI
from openai.types.chat import ChatCompletion, ChatCompletionChunk
from openai.types.embedding import Embedding

# BionicMemoryæ ¸å¿ƒç»„ä»¶
from bionicmemory.core.memory_system import LongShortTermMemorySystem, SourceType
from bionicmemory.services.memory_cleanup_scheduler import MemoryCleanupScheduler
from bionicmemory.core.chroma_service import ChromaService
from bionicmemory.algorithms.newton_cooling_helper import CoolingRate
from bionicmemory.services.local_embedding_service import get_embedding_service

# ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—é…ç½®
from bionicmemory.utils.logging_config import get_logger
logger = get_logger(__name__)

# ========== ç¯å¢ƒå˜é‡é…ç½® ==========
# ç¦ç”¨ChromaDBé¥æµ‹
os.environ["ANONYMIZED_TELEMETRY"] = "False"

API_HOST = os.getenv("API_HOST", "0.0.0.0")
API_PORT = int(os.getenv("API_PORT", "8000"))
CHROMA_HOST = os.getenv("CHROMA_HOST", "localhost")
CHROMA_PORT = int(os.getenv("CHROMA_PORT", "8001"))
CHROMA_CLIENT_TYPE = os.getenv("CHROMA_CLIENT_TYPE", "persistent")

# ========== OpenAIé…ç½® ==========
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE", "https://api.deepseek.com")
OPENAI_MODEL_NAME = os.getenv("OPENAI_MODEL_NAME", "deepseek-chat")

# è®°å¿†ç³»ç»Ÿé…ç½®
SUMMARY_MAX_LENGTH = int(os.getenv('SUMMARY_MAX_LENGTH', '500'))
MAX_RETRIEVAL_RESULTS = int(os.getenv('MAX_RETRIEVAL_RESULTS', '7'))
CLUSTER_MULTIPLIER = int(os.getenv('CLUSTER_MULTIPLIER', '3'))
RETRIEVAL_MULTIPLIER = int(os.getenv('RETRIEVAL_MULTIPLIER', '2'))

# ========== å·¥å…·å‡½æ•° ==========

def extract_user_message(messages: List[Dict]) -> Optional[str]:
    """ä»æ¶ˆæ¯åˆ—è¡¨ä¸­æå–ç”¨æˆ·æ¶ˆæ¯"""
    for message in reversed(messages):  # ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹æŸ¥æ‰¾
        if message.get("role") == "user":
            return message.get("content", "")
    return None

def extract_user_id_from_request(body_data: Dict) -> str:
    """ä»OpenAIè¯·æ±‚ä¸­æå–ç”¨æˆ·ID"""
    try:
        logger.info("ğŸ” å¼€å§‹æå–ç”¨æˆ·ID...")
        
        # 1. ä¼˜å…ˆä»å¯¹è¯åè®®ä¸­çš„userå­—æ®µæå–
        if "user" in body_data:
            raw_user = body_data["user"]
            if isinstance(raw_user, str) and raw_user.strip():
                user_id = raw_user.strip()
                logger.info(f"âœ… ä½¿ç”¨å¯¹è¯åè®®userå­—æ®µ: {user_id}")
                return user_id
        
        # 2. é»˜è®¤å€¼ï¼šdefault_user
        user_id = "default_user"
        logger.info(f"âœ… ä½¿ç”¨é»˜è®¤ç”¨æˆ·ID: {user_id}")
        return user_id
        
    except Exception as e:
        logger.error(f"âŒ æå–ç”¨æˆ·IDå¤±è´¥: {e}")
        return "default_user"

def enhance_chat_with_memory(body_data: Dict, user_id: str) -> Tuple[Dict, List[float]]:
    """
    ä½¿ç”¨è®°å¿†ç³»ç»Ÿå¢å¼ºèŠå¤©è¯·æ±‚
    
    Args:
        body_data: è¯·æ±‚ä½“æ•°æ®
        user_id: ç”¨æˆ·ID
    
    Returns:
        (å¢å¼ºåçš„body_data, enhanced_query_embedding)
    """
    global memory_system
    
    if not memory_system:
        logger.warning("âš ï¸ è®°å¿†ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè·³è¿‡è®°å¿†å¢å¼º")
        return body_data, None
    
    try:
        messages = body_data.get("messages", [])
        if not messages:
            return body_data, None
        
        # æå–ç”¨æˆ·æ¶ˆæ¯
        user_message = extract_user_message(messages)
        if not user_message:
            return body_data, None
        
        # ä½¿ç”¨è®°å¿†ç³»ç»Ÿå¤„ç†ç”¨æˆ·æ¶ˆæ¯
        short_term_records, system_prompt, query_embedding = memory_system.process_user_message(
            user_message, user_id
        )
        
        if short_term_records:
            logger.info(f"ğŸ§  æ‰¾åˆ° {len(short_term_records)} æ¡ç›¸å…³è®°å¿†")
            logger.info(f"ğŸ§  ç”Ÿæˆçš„ç³»ç»Ÿæç¤ºè¯­é•¿åº¦: {len(system_prompt)}")
            
            # ç›´æ¥ä½¿ç”¨memory_systemç”Ÿæˆçš„ç³»ç»Ÿæç¤ºè¯­ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯
            system_message = {
                "role": "system",
                "content": system_prompt
            }
            
            # åœ¨ç”¨æˆ·æ¶ˆæ¯å‰æ’å…¥ç³»ç»Ÿæ¶ˆæ¯
            enhanced_messages = [system_message] + (messages[-3:] if len(messages) > 3 else messages)
            body_data["messages"] = enhanced_messages
            
            logger.info(f"ğŸ§  è®°å¿†å¢å¼ºå®Œæˆï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)} -> {len(enhanced_messages)}")
            logger.info(f"ğŸ§  è®°å¿†å¢å¼ºå®Œæˆï¼Œæ¶ˆæ¯å†…å®¹: {enhanced_messages}")
        
        return body_data, query_embedding
        
    except Exception as e:
        logger.error(f"âŒ è®°å¿†å¢å¼ºå¤±è´¥: {e}")
        return body_data, None

async def process_ai_reply_async(response_content: str, user_id: str, current_user_content: str = None):
    """å¼‚æ­¥å¤„ç†AIå›å¤ï¼ˆä¸é˜»å¡å“åº”æ€§èƒ½ï¼‰"""
    global memory_system
    
    if not memory_system:
        return
    
    try:
        # æ‰§è¡Œè®°å¿†ç³»ç»Ÿå¤„ç†ï¼ˆæ­£ç¡®çš„ä¸šåŠ¡é€»è¾‘é¡ºåºï¼‰
        await memory_system.process_agent_reply_async(response_content, user_id, current_user_content)
        
    except Exception as e:
        logger.error(f"âŒ å¼‚æ­¥å¤„ç†AIå›å¤å¤±è´¥: {e}")

# ========== å…¨å±€å˜é‡ ==========
memory_system = None
memory_cleanup_scheduler = None
chroma_service = None

# OpenAIå®¢æˆ·ç«¯
openai_client = None
async_openai_client = None

# ========== åˆå§‹åŒ–å‡½æ•° ==========

def initialize_memory_system():
    """åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"""
    global memory_system, memory_cleanup_scheduler, chroma_service
    
    try:
        logger.info("æ­£åœ¨åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–ChromaDBæœåŠ¡ï¼ˆåªä½¿ç”¨æœ¬åœ°embeddingï¼‰
        chroma_service = ChromaService()
        logger.info("ChromaDBæœåŠ¡åˆå§‹åŒ–å®Œæˆï¼ˆæœ¬åœ°embeddingæ¨¡å¼ï¼‰")
        
        # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
        memory_system = LongShortTermMemorySystem(
            chroma_service=chroma_service,
            summary_threshold=SUMMARY_MAX_LENGTH,
            max_retrieval_results=MAX_RETRIEVAL_RESULTS,
            cluster_multiplier=CLUSTER_MULTIPLIER,
            retrieval_multiplier=RETRIEVAL_MULTIPLIER,
        )
        
        # å¯åŠ¨æ—¶æ¸…ç©ºçŸ­æœŸè®°å¿†åº“
        try:
            # æ¸…ç©ºçŸ­æœŸè®°å¿†åº“
            short_term_deleted_ids = chroma_service.delete_documents(
                memory_system.short_term_collection_name
            )
            logger.info(f"å¯åŠ¨æ¸…ç©ºçŸ­æœŸè®°å¿†åº“ï¼Œåˆ é™¤ {len(short_term_deleted_ids)} æ¡è®°å½•")
            
        except Exception as _e:
            logger.warning("å¯åŠ¨æ¸…ç©ºçŸ­æœŸè®°å¿†åº“å¤±è´¥", exc_info=True)
        
        # åˆå§‹åŒ–æ¸…ç†è°ƒåº¦å™¨
        memory_cleanup_scheduler = MemoryCleanupScheduler(memory_system=memory_system)
        memory_cleanup_scheduler.start()
        
        logger.info("è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}", exc_info=True)
        return False

def initialize_openai_clients():
    """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
    global openai_client, async_openai_client
    
    try:
        logger.info("æ­£åœ¨åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯...")
        
        # åŒæ­¥å®¢æˆ·ç«¯
        openai_client = OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_API_BASE
        )
        
        # å¼‚æ­¥å®¢æˆ·ç«¯
        async_openai_client = AsyncOpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_API_BASE
        )
        
        logger.info("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

# ========== ç”Ÿå‘½å‘¨æœŸäº‹ä»¶å¤„ç†å™¨ ==========
@asynccontextmanager
async def lifespan(app: FastAPI):
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    initialize_memory_system()
    initialize_openai_clients()
    yield
    # å…³é—­æ—¶æ¸…ç†
    if memory_cleanup_scheduler:
        memory_cleanup_scheduler.stop()
        logger.info("è®°å¿†æ¸…ç†è°ƒåº¦å™¨å·²åœæ­¢")

# ========== FastAPIåº”ç”¨åˆå§‹åŒ– ==========
app = FastAPI(title="BionicMemory OpenAI Proxy", version="2.0.0", lifespan=lifespan)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========== å¥åº·æ£€æŸ¥ç«¯ç‚¹ ==========
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "service": "BionicMemory OpenAI Proxy",
        "timestamp": datetime.now().isoformat(),
        "memory_system_initialized": memory_system is not None,
        "openai_client_initialized": openai_client is not None,
        "cleanup_scheduler_running": memory_cleanup_scheduler is not None if memory_cleanup_scheduler else False
    }

# ========== ä¸»è¦è·¯ç”±å¤„ç† ==========
@app.api_route("/v1/{path:path}", methods=["POST", "GET"])
async def proxy(request: Request, path: str):
    """
    ä»£ç†æ‰€æœ‰ /v1/* è¯·æ±‚
    ä½¿ç”¨OpenAIå®˜æ–¹åº“å¤„ç†ï¼Œç¡®ä¿å®Œå…¨å…¼å®¹
    """
    body = await request.body()
    
    # è®°å½•åŸºæœ¬è¯·æ±‚ä¿¡æ¯
    logger.info(f"ğŸ“¥ æ”¶åˆ°è¯·æ±‚: {request.method} /v1/{path}")
    
    # ========== è·¯ç”±å¤„ç† ==========
    if path.startswith("embeddings"):
        # Embedding API - ä½¿ç”¨æœ¬åœ°embeddingæœåŠ¡
        return await handle_embedding_request(request, path, body)
        
    elif path == "chat/completions":
        # Chat Completions API - ä½¿ç”¨OpenAIå®¢æˆ·ç«¯ + è®°å¿†å¢å¼º
        return await handle_chat_request(request, path, body)
        
    else:
        # å…¶ä»– API - ä½¿ç”¨OpenAIå®¢æˆ·ç«¯é€ä¼ 
        return await handle_other_request(request, path, body)

# ========== å¤„ç†å‡½æ•° ==========

async def handle_embedding_request(request: Request, path: str, body: bytes):
    """å¤„ç†embeddingè¯·æ±‚ - ä½¿ç”¨æœ¬åœ°embeddingæœåŠ¡"""
    try:
        # è§£æè¯·æ±‚ä½“
        if body:
            body_data = json.loads(body)
            input_text = body_data.get("input", "")
            model = body_data.get("model", "")
            
            # ä½¿ç”¨æœ¬åœ°embeddingæœåŠ¡
            logger.info("ä½¿ç”¨æœ¬åœ°embeddingæœåŠ¡")
            embedding_service = get_embedding_service()
            embeddings = embedding_service.get_embeddings([input_text])
            
            # æ„é€ OpenAIå…¼å®¹çš„å“åº”
            response_data = {
                "object": "list",
                "data": [{
                    "object": "embedding",
                    "index": 0,
                    "embedding": embeddings[0]
                }],
                "model": model,
                "usage": {
                    "prompt_tokens": len(input_text.split()),
                    "total_tokens": len(input_text.split())
                }
            }
            
            return JSONResponse(content=response_data)
                
    except Exception as e:
        logger.error(f"âŒ å¤„ç†embeddingè¯·æ±‚å¤±è´¥: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"å¤„ç†embeddingè¯·æ±‚å¤±è´¥: {str(e)}"}
        )

async def handle_chat_request(request: Request, path: str, body: bytes):
    """å¤„ç†å¯¹è¯è¯·æ±‚ - ä½¿ç”¨OpenAIå®¢æˆ·ç«¯ + è®°å¿†å¢å¼º"""
    try:
        # è§£æè¯·æ±‚ä½“
        body_data = None
        user_id = None
        enhanced_query_embedding = None
        current_user_content = None
        
        if body:
            body_data = json.loads(body)
            # æå–ç”¨æˆ·ID
            user_id = extract_user_id_from_request(body_data)
            
            # æ›¿æ¢æ¨¡å‹åç§°
            if "model" in body_data:
                body_data["model"] = OPENAI_MODEL_NAME
            
            # è®°å¿†å¢å¼ºå¤„ç†
            enhanced_body_data, query_embedding = enhance_chat_with_memory(body_data, user_id)
            current_user_content = body_data.get("messages", [])[-1].get("content", "")
            body_data = enhanced_body_data
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæµå¼å“åº”
        is_stream = body_data and body_data.get("stream", False) if body_data else False
        
        if is_stream:
            # æµå¼å“åº” - ä½¿ç”¨å¼‚æ­¥OpenAIå®¢æˆ·ç«¯
            logger.info("ğŸŒŠ å¤„ç†æµå¼å“åº”ï¼ˆä½¿ç”¨OpenAIå®¢æˆ·ç«¯ï¼‰")
            
            try:
                # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯åˆ›å»ºæµå¼å“åº”
                stream = await async_openai_client.chat.completions.create(
                    model=body_data.get("model", OPENAI_MODEL_NAME),
                    messages=body_data.get("messages", []),
                    stream=True,
                    **{k: v for k, v in body_data.items() 
                       if k not in ["model", "messages", "stream"]}
                )
                
                async def openai_stream_wrapper():
                    full_content = ""
                    async for chunk in stream:
                        # ä½¿ç”¨OpenAIåŸç”Ÿæ ¼å¼
                        chunk_data = chunk.model_dump()
                        content = chunk_data.get('choices', [{}])[0].get('delta', {}).get('content', '')
                        if content:
                            full_content += content
                        
                        # è½¬æ¢ä¸ºSSEæ ¼å¼
                        yield f"data: {json.dumps(chunk_data)}\n\n"
                    
                    # æµå¼ç»“æŸåå¼‚æ­¥å­˜å‚¨è®°å¿†
                    if full_content and body_data:
                        asyncio.create_task(process_ai_reply_async(
                            full_content, user_id, current_user_content
                        ))
                    
                    yield "data: [DONE]\n\n"
                
                return StreamingResponse(
                    openai_stream_wrapper(),
                    status_code=200,
                    headers={
                        "Content-Type": "text/plain; charset=utf-8",
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive"
                    }
                )
                
            except Exception as e:
                logger.error(f"âŒ OpenAIæµå¼å¤„ç†å¤±è´¥: {e}")
                return JSONResponse(
                    status_code=500,
                    content={"error": f"æµå¼å¤„ç†å¤±è´¥: {str(e)}"}
                )
        else:
            # éæµå¼å“åº” - ä½¿ç”¨åŒæ­¥OpenAIå®¢æˆ·ç«¯
            logger.info("ğŸ“ å¤„ç†éæµå¼å“åº”ï¼ˆä½¿ç”¨OpenAIå®¢æˆ·ç«¯ï¼‰")
            
            try:
                response = openai_client.chat.completions.create(
                    model=body_data.get("model", OPENAI_MODEL_NAME),
                    messages=body_data.get("messages", []),
                    **{k: v for k, v in body_data.items() 
                       if k not in ["model", "messages"]}
                )
                
                # å¼‚æ­¥å­˜å‚¨è®°å¿†
                if response.choices[0].message.content and body_data:
                    asyncio.create_task(process_ai_reply_async(
                        response.choices[0].message.content, 
                        user_id, 
                        current_user_content
                    ))
                
                # è¿”å›OpenAIåŸç”Ÿå“åº”
                return JSONResponse(content=response.model_dump())
                
            except Exception as e:
                logger.error(f"âŒ OpenAIéæµå¼å¤„ç†å¤±è´¥: {e}")
                return JSONResponse(
                    status_code=500,
                    content={"error": f"éæµå¼å¤„ç†å¤±è´¥: {str(e)}"}
                )
            
    except Exception as e:
        logger.error(f"âŒ å¤„ç†å¯¹è¯è¯·æ±‚å¤±è´¥: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"å¤„ç†å¯¹è¯è¯·æ±‚å¤±è´¥: {str(e)}"}
        )

async def handle_other_request(request: Request, path: str, body: bytes):
    """å¤„ç†å…¶ä»–API - ä½¿ç”¨OpenAIå®¢æˆ·ç«¯é€ä¼ """
    try:
        # è§£æè¯·æ±‚ä½“
        body_data = json.loads(body) if body else {}
        
        # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯å¤„ç†å…¶ä»–è¯·æ±‚
        logger.info(f"ğŸ”„ å¤„ç†å…¶ä»–è¯·æ±‚: {path}")
        
        # æ ¹æ®è·¯å¾„é€‰æ‹©å¤„ç†æ–¹æ³•
        if path == "models":
            # æ¨¡å‹åˆ—è¡¨è¯·æ±‚
            models_response = {
                "object": "list",
                "data": [
                    {
                        "id": OPENAI_MODEL_NAME,
                        "object": "model",
                        "created": int(datetime.now().timestamp()),
                        "owned_by": "bionicmemory"
                    }
                ]
            }
            return JSONResponse(content=models_response)
        
        else:
            # å…¶ä»–è¯·æ±‚é€ä¼ 
            try:
                # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯å¤„ç†
                if request.method == "GET":
                    # GETè¯·æ±‚å¤„ç†
                    response = openai_client._client.get(f"/v1/{path}")
                    return JSONResponse(content=response.json())
                else:
                    # POSTè¯·æ±‚å¤„ç†
                    response = openai_client._client.post(
                        f"/v1/{path}",
                        json=body_data,
                        headers={"Authorization": f"Bearer {OPENAI_API_KEY}"}
                    )
                    return JSONResponse(content=response.json())
                    
            except Exception as e:
                logger.error(f"âŒ OpenAIå®¢æˆ·ç«¯å¤„ç†å…¶ä»–è¯·æ±‚å¤±è´¥: {e}")
                return JSONResponse(
                    status_code=500,
                    content={"error": f"å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}"}
                )
        
    except Exception as e:
        logger.error(f"âŒ å¤„ç†å…¶ä»–è¯·æ±‚å¤±è´¥: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"å¤„ç†å…¶ä»–è¯·æ±‚å¤±è´¥: {str(e)}"}
        )

# ========== å¯åŠ¨é…ç½® ==========
if __name__ == "__main__":
    uvicorn.run(
        "bionicmemory.api.proxy_server_openai:app",
        host=API_HOST,
        port=API_PORT,
        log_level="info",
        access_log=True,
        reload=False
    )
